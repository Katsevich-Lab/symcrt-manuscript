Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Li2021b,
abstract = {This paper develops a method based on model-X knockoffs to find conditional associations that are consistent across diverse environments, controlling the false discovery rate. The motivation for this problem is that large data sets may contain numerous associations that are statistically significant and yet misleading, as they are induced by confounders or sampling imperfections. However, associations consistently replicated under different conditions may be more interesting. In fact, consistency sometimes provably leads to valid causal inferences even if conditional associations do not. While the proposed method is flexible and can be deployed in a wide range of applications, this paper highlights its relevance to genome-wide association studies, in which consistency across populations with diverse ancestries mitigates confounding due to unmeasured variants. The effectiveness of this approach is demonstrated by simulations and applications to the UK Biobank data.},
archivePrefix = {arXiv},
arxivId = {2106.04118},
author = {Li, Shuangning and Sesia, Matteo and Romano, Yaniv and Cand{\`{e}}s, Emmanuel and Sabatti, Chiara},
eprint = {2106.04118},
journal = {Biometrika},
keywords = {FDR,GWAS,causality,conditional independence,false discovery rate,genome-wide association studies,knockoffs},
mendeley-tags = {FDR,GWAS,causality,knockoffs},
number = {3},
pages = {611--629},
title = {{Searching for robust associations with a multi-environment knockoff filter}},
url = {https://academic.oup.com/biomet/article-abstract/109/3/611/6415825?redirectedFrom=fulltext},
volume = {109},
year = {2022}
}
@article{Li2022,
abstract = {The model-X conditional randomization test (CRT) proposed by Cand\`es et al. (2018) is known as a flexible and powerful testing procedure for the conditional independence hypothesis: X is independent of Y conditional on Z. Though having many attractive properties, the model-X CRT relies on the model-X assumption that we have access to perfect knowledge of the distribution of X conditional on Z. If there is a specification error in modeling the distribution of X conditional on Z, this approach may lose its validity. This problem is even more severe when the adjustment covariates Z are of high dimensionality, in which situation precise modeling of X against Z can be hard. In response to this, we propose the Maxway (Model and Adjust X With the Assistance of Y) CRT, a more robust inference approach for conditional independence when the conditional distribution of X is unknown and needs to be estimated from the data. Besides the distribution of X | Z, the Maxway CRT also learns the distribution of Y | Z, using it to calibrate the resampling distribution of X to gain robustness to the error in modeling X. We show that the type-I error inflation of the Maxway CRT can be controlled by the learning error for the low-dimensional adjusting model plus the product of learning errors for the distribution of X | Z and the distribution of Y | Z. This result can be interpreted as an "almost doubly robust" property of the Maxway CRT. Through extensive simulation studies, we demonstrate that the Maxway CRT achieves significantly better type-I error control than existing model-X inference approaches while having similar power. Finally, we apply our methodology to the UK biobank dataset with the goal of studying the relationship between the functional SNP of statins and the risk for type II diabetes mellitus.},
archivePrefix = {arXiv},
arxivId = {2203.06496},
author = {Li, Shuangning and Liu, Molei},
eprint = {2203.06496},
file = {::},
journal = {Journal of the Royal Statistical Society, Series B},
keywords = {conditional independence testing,conditional randomization test,double robustness,doubly robust,learning,semi-supervised},
mendeley-tags = {conditional independence testing,conditional randomization test,double robustness},
number = {5},
pages = {1441--1470},
title = {{Maxway CRT: Improving the Robustness of Model-X Inference}},
url = {http://arxiv.org/abs/2203.06496},
volume = {85},
year = {2023}
}
@article{Andrews2016,
author = {Andrews, Donald W K},
file = {:C\:/Users/abch/Downloads/Inconsistency of the Bootstrap When a Parameter is on the Boundary of the Parameter.pdf:pdf},
number = {2},
pages = {399--405},
title = {{Inconsistency of the Bootstrap When a Parameter is on the Boundary of the Parameter Space Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/2999432 REFERENCES Linked references are available on JSTOR for this article : You ma}},
volume = {68},
year = {2016}
}
@article{Grzenda2008,
author = {Grzenda, Wioletta and Zieba, Wieslaw},
file = {::},
journal = {International Mathematical Forum},
number = {31},
pages = {1521--1528},
title = {{Conditional central limit theorem}},
volume = {3},
year = {2008}
}
@article{Neykov2021,
abstract = {We consider the problem of conditional independence testing of X and Y given Z where X, Y and Z are three real random variables and Z is continuous. We focus on two main cases-when X and Y are both discrete, and when X and Y are both continuous. In view of recent results on conditional independence testing [Ann. Statist. 48 (2020) 1514-1538], one cannot hope to design nontrivial tests, which control the type I error for all absolutely continuous conditionally independent distributions, while still ensuring power against interesting alternatives. Consequently, we identify various, natural smoothness assumptions on the conditional distributions of X, Y |Z = z as z varies in the support of Z, and study the hardness of conditional independence testing under these smoothness assumptions. We derive matching lower and upper bounds on the critical radius of separation between the null and alternative hypotheses in the total variation metric. The tests we consider are easily implementable and rely on binning the support of the continuous variable Z. To complement these results, we provide a new proof of the hardness result of Shah and Peters [Ann. Statist. 48 (2020) 1514-1538].},
archivePrefix = {arXiv},
arxivId = {2001.03039},
author = {Neykov, Matey and Balakrishnan, Sivaraman and Wasserman, Larry},
doi = {10.1214/20-AOS2030},
eprint = {2001.03039},
file = {::},
issn = {21688966},
journal = {Annals of Statistics},
keywords = {Conditional independence,Hypothesis testing,Minimax optimality,conditional independence testing,minimax,optimality},
mendeley-tags = {conditional independence testing,minimax,optimality},
number = {4},
pages = {2151--2177},
title = {{Minimax optimal conditional independence testing}},
volume = {49},
year = {2021}
}
@article{Jankova2018a,
abstract = {Asymptotic lower bounds for estimation play a fundamental role in assessing the quality of statistical procedures. In this paper, we propose a framework for obtaining semiparametric efficiency bounds for sparse high-dimensional models, where the dimension of the parameter is larger than the sample size. We adopt a semiparametric point of view: we concentrate on one-dimensional functions of a high-dimensional parameter. We follow two different approaches to reach the lower bounds: asymptotic Cram{\'{e}}r-Rao bounds and Le Cam's type of analysis. Both of these approaches allow us to define a class of asymptotically unbiased or “regular” estimators for which a lower bound is derived. Consequently, we show that certain estimators obtained by de-sparsifying (or de-biasing) an 1-penalized M-estimator are asymptotically unbiased and achieve the lower bound on the variance: thus in this sense they are asymptotically efficient. The paper discusses in detail the linear regression model and the Gaussian graphical model.},
archivePrefix = {arXiv},
arxivId = {1601.00815},
author = {Jankov{\'{a}}, Jana and {Van De Geer}, Sara},
doi = {10.1214/17-AOS1622},
eprint = {1601.00815},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jankov{\'{a}}, Van De Geer - 2018 - Semiparametric efficiency bounds for high-dimensional models.pdf:pdf;::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Asymptotic efficiency,Cram{\'{e}}r-Rao bound,Graphical models,High-dimensional,Lasso,Le Cam's lemma,Linear regression,Sparsity,high-dimensional regression,semiparametrics},
mendeley-tags = {high-dimensional regression,semiparametrics},
number = {5},
pages = {2336--2359},
title = {{Semiparametric efficiency bounds for high-dimensional models}},
volume = {46},
year = {2018}
}
@article{CetL16,
author = {Cand{\`{e}}s, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cand{\`{e}}s et al. - 2018 - Panning for gold `model-X' knockoffs for high dimensional controlled variable selection(2).pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {FDR,Multiple testing,high-dimensional regression,knockoffs,model-X,variable selection},
mendeley-tags = {FDR,Multiple testing,high-dimensional regression,knockoffs,model-X,variable selection},
number = {3},
pages = {551--577},
publisher = {Wiley Online Library},
title = {{Panning for gold: `model-X' knockoffs for high dimensional controlled variable selection}},
volume = {80},
year = {2018}
}
@article{Dukes2020a,
abstract = {The problem of how to best select variables for confounding adjustment forms one of the key challenges in the evaluation of exposure or treatment effects in observational studies. Routine practice is often based on stepwise selection procedures that use hypothesis testing, change-in-estimate assessments or the lasso, which have all been criticised for – amongst other things – not giving sufficient priority to the selection of confounders. This has prompted vigorous recent activity in developing procedures that prioritise the selection of confounders, while preventing the selection of so-called instrumental variables that are associated with exposure, but not outcome (after adjustment for the exposure). A major drawback of all these procedures is that there is no finite sample size at which they are guaranteed to deliver treatment effect estimators and associated confidence intervals with adequate performance. This is the result of the estimator jumping back and forth between different selected models, and standard confidence intervals ignoring the resulting model selection uncertainty. In this paper, we will develop insight into this by evaluating the finite-sample distribution of the exposure effect estimator in linear regression, under a number of the aforementioned confounder selection procedures. We will show that by making clever use of propensity scores, a simple and generic solution is obtained in the context of generalized linear models, which overcomes this concern (under weaker conditions than competing proposals). Specifically, we propose to use separate regularized regressions for the outcome and propensity score models in order to construct a doubly robust ‘g-estimator'; when these models are sufficiently sparse and correctly specified, standard confidence intervals for the g-estimator implicitly incorporate the uncertainty induced by the variable selection procedure.},
author = {Dukes, Oliver and Vansteelandt, Stijn},
doi = {10.1177/0962280219862005},
file = {::},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {Causal inference,causal inference,causality,double robustness,high-dimensional regression,high-dimensional statistics,model uncertainty,variable selection},
mendeley-tags = {causal inference,causality,double robustness,high-dimensional regression},
number = {3},
pages = {677--694},
pmid = {31385558},
title = {{How to obtain valid tests and confidence intervals after propensity score variable selection?}},
volume = {29},
year = {2020}
}
@article{Ham2022,
abstract = {Conjoint analysis is a popular experimental design used to measure multidimensional preferences. Researchers examine how varying a factor of interest, while controlling for other relevant factors, influences decision-making. Currently, there exist two methodological approaches to analyzing data from a conjoint experiment. The first focuses on estimating the average marginal effects of each factor while averaging over the other factors. Although this allows for straightforward design-based estimation, the results critically depend on the distribution of other factors and how interaction effects are aggregated. An alternative model-based approach can compute various quantities of interest, but requires researchers to correctly specify the model, a challenging task for conjoint analysis with many factors and possible interactions. In addition, a commonly used logistic regression has poor statistical properties even with a moderate number of factors when incorporating interactions. We propose a new hypothesis testing approach based on the conditional randomization test to answer the most fundamental question of conjoint analysis: Does a factor of interest matter in any way given the other factors? Our methodology is solely based on the randomization of factors, and hence is free from assumptions. Yet, it allows researchers to use any test statistic, including those based on complex machine learning algorithms. As a result, we are able to combine the strengths of the existing design-based and model-based approaches. We illustrate the proposed methodology through conjoint analysis of immigration preferences and political candidate evaluation. We also extend the proposed approach to test for regularity assumptions commonly used in conjoint analysis.},
archivePrefix = {arXiv},
arxivId = {2201.08343},
author = {Ham, Dae Woong and Imai, Kosuke and Janson, Lucas},
eprint = {2201.08343},
file = {::},
journal = {arXiv},
keywords = {causal inference,model-X},
mendeley-tags = {causal inference,model-X},
title = {{Using Machine Learning to Test Causal Hypotheses in Conjoint Analysis}},
url = {http://arxiv.org/abs/2201.08343},
year = {2022}
}
@book{Brown1986,
author = {Brown, Lawrence D .},
file = {::},
isbn = {0940600102},
keywords = {exponential families},
mendeley-tags = {exponential families},
title = {{Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory}},
year = {1986}
}
@article{Doran2014,
abstract = {Determining conditional independence (CI) relationships between random variables is a challenging but important task for problems such as Bayesian network learning and causal discovery. We propose a new kernel CI test that uses a single, learned permutation to convert the CI test problem into an easier two-sample test problem. The learned permutation leaves the joint distribution unchanged if and only if the null hypothesis of CI holds. Then, a kernel two-sample test, which has been studied extensively in prior work, can be applied to a permuted and an unpermuted sample to test for CI. We demonstrate that the test (1) easily allows the incorporation of prior knowledge during the permutation step, (2) has power competitive with state-of-the-art kernel CI tests, and (3) accurately estimates the null distribution of the test statistic, even as the dimensionality of the conditioning variable grows.},
author = {Doran, Gary and Muandet, Krikamol and Zhang, Kun and Sch{\"{o}}lkopf, Bernhard},
file = {::},
isbn = {9780974903910},
journal = {Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014},
pages = {132--141},
title = {{A permutation-based kernel conditional independence test}},
year = {2014}
}
@book{Bickel1993,
address = {Baltimore},
author = {Bickel, P.J. and Klaassen, C.A. and Ritov, Y.A. and Wellner, J.A.},
file = {::},
keywords = {semiparametrics},
mendeley-tags = {semiparametrics},
publisher = {Johns Hopkins University Press},
title = {{Efficient and Adaptive Estimation for Semiparametric Models}},
year = {1993}
}
@article{Choi1996,
abstract = {Tests of hypotheses about finite-dimensional parameters in a semiparametric model are studied from Pitman's moving alternative (or local) approach using Le Cam's local asymptotic normality concept. For the case of a real parameter being tested, asymptotically uniformly most powerful (AUMP) tests are characterized for one-sided hypotheses, and AUMP unbiased tests for two-sided ones. An asymptotic invariance principle is introduced for multidimensional hypotheses, and AUMP invariant tests are characterized. These provide optimality for Wald, Rao (score), Neyman-Rao (effective score) and likelihood ratio tests in parametric models, and for Neyman-Rao tests in semiparametric models when constructions are feasible. Inversions lead to asymptotically uniformly most accurate confidence sets. Examples include one-, two-and k-sample problems, a linear regression model with unknown error distribution and a proportional hazards regression model with arbitrary baseline hazards. Results are presented in a format that facilitates application in strictly parametric models.},
author = {Choi, Sungsub and Hall, W. J. and Schick, Anton},
doi = {10.1214/aos/1032894469},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Hall, Schick - 1996 - Asymptotically uniformly most powerful tests in parametric and semiparametric models.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Adaptation,Asymptotic confidence sets,Effective scores,Efficient tests,Invariance,Local alternatives,Unbiased tests,semiparametrics,testing},
mendeley-tags = {semiparametrics,testing},
number = {2},
pages = {841--861},
title = {{Asymptotically uniformly most powerful tests in parametric and semiparametric models}},
volume = {24},
year = {1996}
}
@article{Vansteelandt2008,
abstract = {A primary focus of an increasing number of scientific studies is to determine whether two exposures interact in the effect that they produce on an outcome of interest. Interaction is commonly assessed by fitting regression models in which the linear predictor includes the product between those exposures. When the main interest lies in the interaction, this approach is not entirely satisfactory, because it is prone to (possibly severe) bias when the main exposure effects or the associations between outcome and extraneous factors are misspecified. In this article we consider conditional mean models with identity or log link that postulate the statistical interaction in terms of a finite-dimensional parameter but are otherwise unspecified. We show that estimation of the interaction parameter often is not feasible in this model, because it requires nonparametric estimation of auxiliary conditional expectations given high-dimensional variables.We thus consider multiply robust estimation under a union model that assumes that at least one of several working submodels holds. Our approach is novel in that it makes use of information on the joint distribution of the exposures conditional on the extraneous factors in making inferences about the interaction parameter of interest. In the special case of a randomized trial or a family-based genetic study in which the joint exposure distribution is known by design or by Mendelian inheritance, the resulting multiply robust procedure leads to asymptotically distribution-free tests of the null hypothesis of no interaction on an additive scale. We illustrate the methods through simulation and analysis of a randomized follow-up study. {\textcopyright} 2008 American Statistical Association.},
author = {Vansteelandt, Stijn and Vanderweele, Tyler J. and Tchetgen, Eric J. and Robins, James M.},
doi = {10.1198/016214508000001084},
file = {::},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Double robustness,Gene-environment interaction,Gene-gene interaction,Longitudinal data,Semiparametric inference,double robustness,interactions},
mendeley-tags = {double robustness,interactions},
number = {484},
pages = {1693--1704},
title = {{Multiply robust inference for statistical interactions}},
volume = {103},
year = {2008}
}
@article{Fan2018a,
abstract = {Interpretability and stability are two important features that are desired in many contemporary big data applications arising in economics and finance. While the former is enjoyed to some extent by many existing forecasting approaches, the latter in the sense of controlling the fraction of wrongly discovered features which can enhance greatly the interpretability is still largely underdeveloped in the econometric settings. To this end, in this paper we exploit the general framework of model-X knockoffs introduced recently in Cand{\`{e}}s, Fan, Janson and Lv (2018), which is nonconventional for reproducible large-scale inference in that the framework is completely free of the use of p-values for significance testing, and suggest a new method of intertwined probabilistic factors decoupling (IPAD) for stable interpretable forecasting with knockoffs inference in high-dimensional models. The recipe of the method is constructing the knockoff variables by assuming a latent factor model that is exploited widely in economics and finance for the association structure of covariates. Our method and work are distinct from the existing literature in that we estimate the covariate distribution from data instead of assuming that it is known when constructing the knockoff variables, our procedure does not require any sample splitting, we provide theoretical justifications on the asymptotic false discovery rate control, and the theory for the power analysis is also established. Several simulation examples and the real data analysis further demonstrate that the newly suggested method has appealing finite-sample performance with desired interpretability and stability compared to some popularly used forecasting methods. Running title: IPAD},
author = {Fan, Yingying and Lv, Jinchi and Sharifvaghefi, Mahrad and Uematsu, Yoshimasa},
doi = {10.2139/ssrn.3245137},
file = {::},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Multiple testing,applied,fdr,knockoffs,large-scale inference and,latent factors,model-x,to-skim},
mendeley-tags = {Multiple testing,applied,knockoffs,to-skim},
number = {532},
pages = {1822--1834},
publisher = {Taylor & Francis},
title = {{IPAD: Stable Interpretable Forecasting with Knockoffs Inference}},
url = {https://doi.org/10.1080/01621459.2019.1654878},
volume = {115},
year = {2020}
}
@article{Zhang2011,
abstract = {Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.},
archivePrefix = {arXiv},
arxivId = {1202.3775},
author = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Sch{\"{o}}lkopf, Bernhard},
eprint = {1202.3775},
file = {::},
journal = {Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, UAI 2011},
pages = {804--813},
title = {{Kernel-based conditional independence test and application in causal discovery}},
year = {2011}
}
@article{Robins2001,
author = {Robins, James M. and Rotnitzky, Andrea},
file = {::},
journal = {Statistica Sinica},
keywords = {causality},
mendeley-tags = {causality},
number = {4},
pages = {920--936},
title = {{Comment on the Bickel and Kwon article, "Inference for semiparametric models: Some questions and an answer"}},
volume = {11},
year = {2001}
}
@article{Berrett2019,
abstract = {We propose a general new method, the conditional permutation test, for testing the conditional independence of variables X and Y given a potentially high-dimensional random vector Z that may contain confounding factors. The proposed test permutes entries of X non-uniformly, so as to respect the existing dependence between X and Z and thus account for the presence of these con-founders. Like the conditional randomization test of Cand{\`{e}}s et al. [7], our test relies on the availability of an approximation to the distribution of X|Z-while Cand{\`{e}}s et al. [7]'s test uses this estimate to draw new X values, for our test we use this approximation to design an appropriate non-uniform distribution on permutations of the X values already seen in the true data. We provide an efficient Markov Chain Monte Carlo sampler for the implementation of our method, and establish bounds on the Type I error in terms of the error in the approximation of the conditional distribution of X|Z, finding that, for the worst case test statistic, the inflation in Type I error of the conditional permutation test is no larger than that of the conditional randomization test. We validate these theoretical results with experiments on simulated data and on the Capital Bikeshare data set.},
author = {Berrett, Thomas B and Wang, Yi and {Foygel Barber}, Rina and Samworth, Richard J},
file = {::},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {conditional permutation test,high-dimensional regression,multiple testing,resampling,unpublished},
mendeley-tags = {conditional permutation test,high-dimensional regression,multiple testing,resampling,unpublished},
number = {1},
pages = {175--197},
title = {{The conditional permutation test for independence while controlling for confounders}},
volume = {82},
year = {2020}
}
@book{Pearl2009,
author = {Pearl, Judea},
publisher = {Cambridge University Press},
title = {{Causality}},
year = {2009}
}
@article{Kennedy2021,
abstract = {Estimators based on influence functions (IFs) have been shown to be effective in many settings, especially when combined with machine learning techniques. By focusing on estimating a specific target of interest (e.g., the average effect of a treatment), rather than on estimating the full underlying data generating distribution, IF-based estimators are often able to achieve asymptotically optimal mean-squared error. Still, many researchers find IF-based estimators to be opaque or overly technical, which makes their use less prevalent and their benefits less available. To help foster understanding and trust in IF-based estimators, we present tangible, visual illustrations of when and how IF-based estimators can outperform standard “plug-in” estimators. The figures we show are based on connections between IFs, gradients, linear approximations, and Newton–Raphson.},
archivePrefix = {arXiv},
arxivId = {1810.03260},
author = {Fisher, Aaron and Kennedy, Edward H.},
doi = {10.1080/00031305.2020.1717620},
eprint = {1810.03260},
file = {::},
issn = {15372731},
journal = {American Statistician},
keywords = {Bias correction,Nonparametric efficiency,Visualization,double robustness},
mendeley-tags = {double robustness},
number = {2},
pages = {162--172},
publisher = {Taylor & Francis},
title = {{Visually Communicating and Teaching Intuition for Influence Functions}},
url = {https://doi.org/10.1080/00031305.2020.1717620},
volume = {75},
year = {2021}
}
@article{Wang2020b,
abstract = {In many scientific problems, researchers try to relate a response variable $Y$ to a set of potential explanatory variables $X = (X_1,\dots,X_p)$, and start by trying to identify variables that contribute to this relationship. In statistical terms, this goal can be posed as trying to identify $X_j$'s upon which $Y$ is conditionally dependent. Sometimes it is of value to simultaneously test for each $j$, which is more commonly known as variable selection. The conditional randomization test (CRT) and model-X knockoffs are two recently proposed methods that respectively perform conditional independence testing and variable selection by, for each $X_j$, computing any test statistic on the data and assessing that test statistic's significance by comparing it to test statistics computed on synthetic variables generated using knowledge of $X$'s distribution. Our main contribution is to analyze their power in a high-dimensional linear model where the ratio of the dimension $p$ and the sample size $n$ converge to a positive constant. We give explicit expressions of the asymptotic power of the CRT, variable selection with CRT $p$-values, and model-X knockoffs, each with a test statistic based on either the marginal covariance, the least squares coefficient, or the lasso. One useful application of our analysis is the direct theoretical comparison of the asymptotic powers of variable selection with CRT $p$-values and model-X knockoffs; in the instances with independent covariates that we consider, the CRT provably dominates knockoffs. We also analyze the power gain from using unlabeled data in the CRT when limited knowledge of $X$'s distribution is available, and the power of the CRT when samples are collected retrospectively.},
author = {Wang, Wenshuo and Janson, Lucas},
file = {::},
journal = {Biometrika},
keywords = {approximate message passing,benjamini,conditional randomization test,conditional randomization testing,hochberg,knockoffs,model-X,model-x,power analysis,retrospective,sampling},
mendeley-tags = {conditional randomization test,knockoffs,model-X,power analysis},
number = {3},
pages = {631--645},
title = {{A high-dimensional Power Analysis of the Conditional Randomization Test and Knockoffs}},
url = {http://arxiv.org/abs/2010.02304},
volume = {109},
year = {2022}
}
@article{Weinstein2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2007.15346v1},
author = {Weinstein, Asaf and Su, Weijie J and Bogdan, Malgorzata and Barber, Rina F and Candes, Emmanuel J},
eprint = {arXiv:2007.15346v1},
file = {::},
journal = {The Annals of Statistics},
keywords = {AMP,high-dimensional regression,knockoffs},
mendeley-tags = {AMP,high-dimensional regression,knockoffs},
number = {3},
pages = {1005--1029},
title = {{A power analysis for model-X knockoffs with l p-regularized statistics}},
volume = {51},
year = {2023}
}
@techreport{Swanson2019,
author = {Swanson, Jason},
file = {::},
title = {{Lecture notes on probability theory}},
url = {http://math.swansonsite.com/19s6245notes.pdf},
year = {2019}
}
@article{Saeki1996,
author = {Saeki, Sadahiro},
doi = {10.1080/00029890.1996.12004804},
file = {::},
issn = {0002-9890},
journal = {The American Mathematical Monthly},
number = {8},
pages = {682--683},
title = {{A Proof of the Existence of Infinite Product Probability Measures}},
volume = {103},
year = {1996}
}
@article{Sen2017,
abstract = {We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution f(x, y, z) of continuous random vectors X, Y and Z, we determine whether X | Y | Z. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution fCI(x, y, z) = f(x|z)f(y|z)f(z) - the joint distribution if and only if X | Y |Z. - when given access only to i.i.d. samples from the true joint distribution f(x, y, z). To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to fCI in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d near-independent samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.},
archivePrefix = {arXiv},
arxivId = {1709.06138},
author = {Sen, Rajat and Suresh, Ananda Theertha and Shanmugam, Karthikeyan and Dimakis, Alexandros G. and Shakkottai, Sanjay},
eprint = {1709.06138},
file = {::},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {conditional independence testing},
mendeley-tags = {conditional independence testing},
number = {Nips},
pages = {2952--2962},
title = {{Model-powered conditional independence test}},
volume = {2017-Decem},
year = {2017}
}
@book{RL2005,
author = {Romano, Joseph P. and Lehmann, E. L.},
booktitle = {Book},
doi = {10.2307/2332982},
file = {::},
isbn = {0387988645},
issn = {00063444},
title = {{Testing Statistical Hypothesis.}},
year = {2005}
}
@article{Emmenegger2021a,
abstract = {We estimate the linear coefficient in a partially linear model with confounding variables. We rely on double machine learning (DML) and extend it with an additional regularization and selection scheme. We allow for more general dependence structures among the model variables than what has been investigated previously, and we prove that this DML estimator remains asymptotically Gaussian and converges at the parametric rate. The DML estimator has a two-stage least squares interpretation and may produce overly wide confidence intervals. To address this issue, we propose the regularization-selection regsDML method that leads to narrower confidence intervals. It is fully data driven and optimizes an estimated asymptotic mean squared error of the coefficient estimate. Empirical examples demonstrate our methodological and theoretical developments. Software code for our regsDML method will be made available in the R-package dmlalg.},
archivePrefix = {arXiv},
arxivId = {2101.12525},
author = {Emmenegger, Corinne and B{\"{u}}hlmann, Peter},
doi = {10.1214/21-ejs1931},
eprint = {2101.12525},
file = {::},
journal = {Electronic Journal of Statistics},
keywords = {causal inference,double machine learning,double robustness,endogenous variables,generalized method of mo-,instrumental variables,k-class estimation,ments,partially linear model,regularization,semiparametric estimation,two-stage least squares},
mendeley-tags = {causal inference,double robustness},
number = {2},
pages = {1--87},
title = {{Regularizing double machine learning in partially linear endogenous models}},
volume = {15},
year = {2021}
}
@article{Hemerik2019a,
abstract = {Generalized linear models are often misspecified due to overdispersion, heteroscedasticity and ignored nuisance variables. Existing quasi-likelihood methods for testing in misspecified models often do not provide satisfactory type-I error rate control. We provide a novel semi-parametric test, based on sign-flipping individual score contributions. The tested parameter is allowed to be multi-dimensional and even high-dimensional. Our test is often robust against the mentioned forms of misspecification and provides better type-I error control than its competitors. When nuisance parameters are estimated, our basic test becomes conservative. We show how to take nuisance estimation into account to obtain an asymptotically exact test. Our proposed test is asymptotically equivalent to its parametric counterpart.},
archivePrefix = {arXiv},
arxivId = {1909.03796},
author = {Hemerik, Jesse and Goeman, Jelle J and Finos, Livio},
eprint = {1909.03796},
file = {::},
journal = {Journal of the Royal Statistical Society, Series B},
keywords = {bootstrap,generalized linear models,glm,high-dimensional,high-dimensional regression,permutation,robust,score,semi-parametric,sign-flipping,test},
mendeley-tags = {bootstrap,generalized linear models,high-dimensional regression},
number = {3},
pages = {841--864},
title = {{Robust testing in generalized linear models by sign-flipping score contributions}},
url = {http://arxiv.org/abs/1909.03796},
volume = {82},
year = {2020}
}
@article{Yuan2014,
abstract = {A conditional version of the classical central limit theorem is derived rigorously by using conditional characteristic functions, and a more general version of conditional central limit theorem for the case of conditionally independent but no necessarily conditionally identically distributed random variables is establtished. These are done anticipating that the field of conditional limit theory will prove to be of significant applicability. {\textcopyright} 2014 The Korean Mathematical Society.},
author = {Yuan, De Mei and Wei, Li Ran and Lei, Lan},
doi = {10.4134/JKMS.2014.51.1.001},
file = {::},
issn = {03049914},
journal = {Journal of the Korean Mathematical Society},
keywords = {Conditional central limit theorem,Conditional characteristic function,Conditional identical distribution,Conditional independence},
number = {1},
pages = {1--15},
title = {{Conditional central limit theorems for a sequence of conditional independent random variables}},
volume = {51},
year = {2014}
}
@article{Scheidegger2022,
abstract = {We introduce a new test for conditional independence which is based on what we call the weighted generalised covariance measure (WGCM). It is an extension of the recently introduced generalised covariance measure (GCM). To test the null hypothesis of X and Y being conditionally independent given Z, our test statistic is a weighted form of the sample covariance between the residuals of nonlinearly regressing X and Y on Z. We propose different variants of the test for both univariate and multivariate X and Y. We give conditions under which the tests yield the correct type I error rate. Finally, we compare our novel tests to the original GCM using simulation and on real data sets. Typically, our tests have power against a wider class of alternatives compared to the GCM. This comes at the cost of having less power against alternatives for which the GCM already works well.},
archivePrefix = {arXiv},
arxivId = {2111.04361},
author = {Scheidegger, Cyrill and H{\"{o}}rrmann, Julia and B{\"{u}}hlmann, Peter},
eprint = {2111.04361},
file = {::},
keywords = {conditional independence testing,conditional independence tests,nonparametric regression,weighted covariance},
mendeley-tags = {conditional independence testing},
pages = {1--68},
title = {{The Weighted Generalised Covariance Measure}},
url = {http://arxiv.org/abs/2111.04361},
volume = {23},
year = {2022}
}
@article{VanDeGeer2014,
abstract = {We propose a general method for constructing confidence intervals and statistical tests for single or low-dimensional components of a large parameter vector in a high-dimensional model. It can be easily adjusted for multiplicity taking dependence among tests into account. For linear models, our method is essentially the same as in Zhang and Zhang [J. R. Stat. Soc. Ser. B Stat. Methodol. 76 (2014) 217-242]: we analyze its asymptotic properties and establish its asymptotic optimality in terms of semiparametric efficiency. Our method naturally extends to generalized linear models with convex loss functions. We develop the corresponding theory which includes a careful analysis for Gaussian, sub-Gaussian and bounded correlated designs.},
author = {{Van De Geer}, Sara and B{\"{u}}hlmann, Peter and Ritov, Ya'acov and Dezeure, Ruben},
doi = {10.1214/14-AOS1221},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Central limit theorem,Generalized linear model,Lasso,Linear model,Multiple testing,Semiparametric efficiency,Sparsity},
number = {3},
pages = {1166--1202},
title = {{On asymptotically optimal confidence regions and tests for high-dimensional models}},
volume = {42},
year = {2014}
}
@article{Henmi2004,
author = {Henmi, Masayuki and Eguchi, Shinto},
file = {::},
journal = {Biometrika},
number = {4},
pages = {929--941},
title = {{A Paradox concerning Nuisance Parameters and Projected Estimating Functions}},
volume = {91},
year = {2004}
}
@article{Zhong2021,
abstract = {We propose a new method named the Conditional Randomization Rank Test (CRRT) for testing conditional independence of a response variable Y and a covariate variable X, conditional on the rest of the covariates Z. The new method generalizes the Conditional Randomization Test (CRT) of [CFJL18] by exploiting the knowledge of the conditional distribution of X|Z and is a conditional sampling based method that is easy to implement and interpret. In addition to guaranteeing exact type 1 error control, owing to a more flexible framework, the new method markedly outperforms the CRT in computational efficiency. We establish bounds on the probability of type 1 error in terms of total variation norm and also in terms of observed Kullback-Leibler divergence when the conditional distribution of X|Z is misspecified. We validate our theoretical results by extensive simulations and show that our new method has considerable advantages over other existing conditional sampling based methods when we take both power and computational efficiency into consideration.},
archivePrefix = {arXiv},
arxivId = {2112.00258},
author = {Zhong, Yanjie and Kuffner, Todd and Lahiri, Soumendra},
eprint = {2112.00258},
file = {::},
journal = {arXiv},
keywords = {MX},
mendeley-tags = {MX},
title = {{Conditional Randomization Rank Test}},
url = {http://arxiv.org/abs/2112.00258},
year = {2021}
}
@article{Fan2020,
abstract = {Power and reproducibility are key to enabling refined scientific discoveries in contemporary big data applications with general high-dimensional nonlinear models. In this article, we provide theoretical foundations on the power and robustness for the model-X knockoffs procedure introduced recently in Cand{\`{e}}s, Fan, Janson and Lv in high-dimensional setting when the covariate distribution is characterized by Gaussian graphical model. We establish that under mild regularity conditions, the power of the oracle knockoffs procedure with known covariate distribution in high-dimensional linear models is asymptotically one as sample size goes to infinity. When moving away from the ideal case, we suggest the modified model-X knockoffs method called graphical nonlinear knockoffs (RANK) to accommodate the unknown covariate distribution. We provide theoretical justifications on the robustness of our modified procedure by showing that the false discovery rate (FDR) is asymptotically controlled at the target level and the power is asymptotically one with the estimated covariate distribution. To the best of our knowledge, this is the first formal theoretical result on the power for the knockoffs procedure. Simulation results demonstrate that compared to existing approaches, our method performs competitively in both FDR control and power. A real dataset is analyzed to further assess the performance of the suggested knockoffs procedure. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1709.00092},
author = {Fan, Yingying and Demirkaya, Emre and Li, Gaorong and Lv, Jinchi},
doi = {10.1080/01621459.2018.1546589},
eprint = {1709.00092},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Big data,Graphical nonlinear knockoffs,High-dimensional nonlinear models,Large-scale inference and FDR,Power,Reproducibility,Robustness},
month = {jan},
number = {529},
pages = {362--379},
publisher = {American Statistical Association},
title = {{RANK: Large-Scale Inference With Graphical Nonlinear Knockoffs}},
volume = {115},
year = {2020}
}
@book{Durrett2010,
abstract = {This lively introduction to measure-theoretic probability theory covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. Concentrating on results that are the most useful for applications, this comprehensive treatment is a rigorous graduate text and reference. Operating under the philosophy that the best way to learn probability is to see it in action, the book contains extended examples that apply the theory to concrete applications. This fifth edition contains a new chapter on multidimensional Brownian motion and its relationship to partial differential equations (PDEs), an advanced topic that is finding new applications. Setting the foundation for this expansion, Chapter 7 now features a proof of It{\^{o}}'s formula. Key exercises that previously were simply proofs left to the reader have been directly inserted into the text as lemmas. The new edition re-instates discussion about the central limit theorem for martingales and stationary sequences.},
author = {Durrett, Rick},
booktitle = {Probability: Theory and Examples},
doi = {10.1017/9781108591034},
edition = {4th},
file = {::},
isbn = {9781108591034},
publisher = {Cambridge University Press},
title = {{Probability: Theory and Examples}},
year = {2010}
}
@article{Bayati2011,
abstract = {We consider the problem of learning a coefficient vector x 0 ∈ ℝ N from noisy linear observation y = Ax 0 + w ∈ ℝ n. In many contexts (ranging from model selection to image processing), it is desirable to construct a sparse estimator x̂. In this case, a popular approach consists in solving an ℓ 1 -penalized least-squares problem known as the LASSO or basis pursuit denoising. For sequences of matrices of increasing dimensions,with independent Gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic mean square error of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical model ideas. Simulations on real data matrices suggest that our results can be relevant in a broad array of practical applications. {\textcopyright} 2006 IEEE.},
archivePrefix = {arXiv},
arxivId = {1008.2581},
author = {Bayati, Mohsen and Montanari, Andrea},
doi = {10.1109/TIT.2011.2174612},
eprint = {1008.2581},
file = {::},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Compressed sensing,graphical models,message passing algorithms,random matrix theory,state evolution,statistical learning},
number = {4},
pages = {1997--2017},
publisher = {IEEE},
title = {{The LASSO risk for Gaussian matrices}},
volume = {58},
year = {2011}
}
@article{Romano2019a,
abstract = {This paper introduces a machine for sampling approximate model-X knockoffs for arbitrary and unspecified data distributions using deep generative models. The main idea is to iteratively refine a knockoff sampling mechanism until a criterion measuring the validity of the produced knockoffs is optimized; this criterion is inspired by the popular maximum mean discrepancy in machine learning and can be thought of as measuring the distance to pairwise exchangeability between original and knockoff features. By building upon the existing model-X framework, we thus obtain a flexible and model-free statistical tool to perform controlled variable selection. Extensive numerical experiments and quantitative tests confirm the generality, effectiveness, and power of our deep knockoff machines. Finally, we apply this new method to a real study of mutations linked to changes in drug resistance in the human immunodeficiency virus.},
author = {Romano, Yaniv and Sesia, Matteo and Cand{\`{e}}s, Emmanuel},
doi = {10.1080/01621459.2019.1660174},
file = {::},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {FDR,Variable selection,false discovery rate,generative models,high-dimensional regression,knockoffs,neural networks,nonparametric methods,variable selection},
mendeley-tags = {FDR,high-dimensional regression,knockoffs,variable selection},
number = {532},
pages = {1861--1872},
publisher = {Taylor & Francis},
title = {{Deep Knockoffs}},
url = {http://dx.doi.org/10.1080/01621459.2019.1660174},
volume = {115},
year = {2019}
}
@article{Kaufmann1985,
author = {Kaufmann, Ludwig Fahrmeir and Heinz},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann - 1985 - Consistency and Asymptotic Normality of the Maximum Likelihood Estimator in Generalized Linear Models.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {asymptotics,generalized linear models},
mendeley-tags = {asymptotics,generalized linear models},
number = {1},
pages = {342--368},
title = {{Consistency and Asymptotic Normality of the Maximum Likelihood Estimator in Generalized Linear Models}},
volume = {13},
year = {1985}
}
@article{SetC17,
abstract = {In this paper we deepen and enlarge the reflection on the possible advantages of a knockoff approach to genome wide association studies (Sesia et al., 2018), starting from the discussions in Bottolo & Richardson (2019); Jewell & Witten (2019); Rosenblatt et al. (2019) and Marchini (2019). The discussants bring up a number of important points, either related to the knockoffs methodology in general, or to its specific application to genetic studies. In the following we offer some clarifications, mention relevant recent developments and highlight some of the still open problems.},
annote = {Forthcoming, preprint arXiv:1706.04677},
author = {Sesia, M. and Sabatti, C. and Cand{\`{e}}s, E. J.},
doi = {10.1093/biomet/asy033},
file = {::;::},
issn = {14643510},
journal = {Biometrika},
keywords = {FDR,False discovery rate,Genome-wide association study,Knockoff,Multiple testing,Variable selection,genetics,high-dimensional regression,knockoffs,model-X},
mendeley-tags = {FDR,Multiple testing,genetics,high-dimensional regression,knockoffs,model-X},
number = {1},
pages = {1--18},
title = {{Gene hunting with hidden Markov model knockoffs}},
volume = {106},
year = {2019}
}
@article{Shekhar2022a,
archivePrefix = {arXiv},
arxivId = {arXiv:2211.14908v1},
author = {Shekhar, Shubhanshu and Kim, Ilmun and Ramdas, Aaditya},
eprint = {arXiv:2211.14908v1},
file = {::},
journal = {Advances in Neural Information Processing Systems},
pages = {18168--18180},
title = {{A permutation-free kernel two-sample test}},
volume = {35},
year = {2022}
}
@article{Ning2017,
abstract = {We consider the problem of uncertainty assessment for low dimensional components in high dimensional models. Specifically, we propose a novel decorrelated score function to handle the impact of high dimensional nuisance parameters. We consider both hypothesis tests and confidence regions for generic penalized M-estimators. Unlike most existing inferential methods which are tailored for individual models, our method provides a general framework for high dimensional inference and is applicable to a wide variety of applications. In particular, we apply this general framework to study five illustrative examples: linear regression, logistic regression, Poisson regression, Gaussian graphical model and additive hazards model. For hypothesis testing, we develop general theorems to characterize the limiting distributions of the decorrelated score test statistic under both null hypothesis and local alternatives. These results provide asymptotic guarantees on the type I errors and local powers. For confidence region construction, we show that the decorrelated score function can be used to construct point estimators that are asymptotically normal and semiparametrically efficient. We further generalize this framework to handle the settings of misspecified models. Thorough numerical results are provided to back up the developed theory.},
archivePrefix = {arXiv},
arxivId = {1412.8765},
author = {Ning, Yang and Liu, Han},
doi = {10.1214/16-AOS1448},
eprint = {1412.8765},
file = {::;::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Confidence interval,High dimensional inference,Hypothesis test,Model misspecification,Nuisance parameter,Score function,Sparsity},
number = {1},
pages = {158--195},
title = {{A general theory of hypothesis tests and confidence regions for sparse high dimensional models}},
volume = {45},
year = {2017}
}
@article{Katsevich2020c,
author = {Barry, Timothy and Wang, Xuran and Morris, John A. and Roeder, Kathryn and Katsevich, Eugene},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barry et al. - 2021 - Conditional resampling improves calibration and sensitivity in single-cell CRISPR screen analysis.pdf:pdf},
journal = {Genome Biology},
keywords = {CRISPR,gene-enhancer},
mendeley-tags = {CRISPR,gene-enhancer},
pages = {1--19},
title = {{SCEPTRE improves calibration and sensitivity in single-cell CRISPR screen analysis}},
url = {https://doi.org/10.1101/2020.08.13.250092},
volume = {22},
year = {2021}
}
@article{Shah2018b,
abstract = {We propose a framework for constructing goodness-of-fit tests in both low and high dimensional linear models. We advocate applying regression methods to the scaled residuals following either an ordinary least squares or lasso fit to the data, and using some proxy for prediction error as the final test statistic. We call this family residual prediction tests. We show that simulation can be used to obtain the critical values for such tests in the low dimensional setting and demonstrate using both theoretical results and extensive numerical studies that some form of the parametric bootstrap can do the same when the high dimensional linear model is under consideration. We show that residual prediction tests can be used to test for significance of groups or individual variables as special cases, and here they compare favourably with state of the art methods, but we also argue that they can be designed to test for as diverse model misspecifications as heteroscedasticity and non-linearity.},
archivePrefix = {arXiv},
arxivId = {1511.03334},
author = {Shah, Rajen D. and B{\"{u}}hlmann, Peter},
doi = {10.1111/rssb.12234},
eprint = {1511.03334},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shah, B{\"{u}}hlmann - 2018 - Goodness-of-fit tests for high dimensional linear models.pdf:pdf},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Bootstrap,Diagnostics,Goodness of fit,High dimensional models,Lasso,goodness of fit,high-dimensional regression},
mendeley-tags = {goodness of fit,high-dimensional regression},
number = {1},
pages = {113--135},
title = {{Goodness-of-fit tests for high dimensional linear models}},
volume = {80},
year = {2018}
}
@article{Kim2020a,
abstract = {Classical asymptotic theory for statistical hypothesis testing, for example Wilks' theorem for likelihood ratios, usually involves calibrating the test statistic by fixing the dimension d while letting the sample size n increase to infinity. In the last few decades, a great deal of effort has been dedicated towards understanding how these methods behave in high-dimensional settings, where dn and n both increase to infinity together at some prescribed relative rate. This often leads to different tests in the two settings, depending on the assumptions about the dimensionality. This leaves the practitioner in a bind: given a dataset with 100 samples in 20 dimensions, should they calibrate by assuming n >> d, or dn/n ≈ 0.2? This paper considers the goal of dimension-agnostic inference—developing methods whose validity does not depend on any assumption on dn. We describe one generic approach that uses variational representations of existing test statistics along with sample-splitting and self-normalization (studentization) to produce a Gaussian limiting null distribution. We exemplify this technique for a handful of classical problems, such as one-sample mean testing, testing if a covariance matrix equals the identity, and kernel methods for testing equality of distributions using degenerate U-statistics like the maximum mean discrepancy. Without explicitly targeting the high-dimensional setting, our tests are shown to be minimax rate-optimal, meaning that the power of our tests cannot be improved further up to a constant factor. A hidden advantage is that our proofs are simple and transparent. We end by describing several fruitful open directions.},
archivePrefix = {arXiv},
arxivId = {2011.05068},
author = {Kim, Ilmun and Ramdas, Aaditya},
eprint = {2011.05068},
file = {::},
issn = {23318422},
journal = {Bernoulli},
number = {1},
pages = {683--711},
title = {{Dimension-agnostic inference}},
volume = {30},
year = {2024}
}
@book{Lista2017,
abstract = {The main goal of an experimental physicist is to measure quantities of interest, possibly with the best precision. In the luckiest cases, measurements lead to the discovery of new physical phenomena that may represent a breakthrough in the knowledge of Nature. Measurements, and, more in general, observations of Nature's behavior, are performed with experiments that record quantitative information about the physical phenomenon under observation.},
author = {Klenke, Achim},
booktitle = {Lecture Notes in Physics},
file = {::},
isbn = {9781447153603},
issn = {00758450},
pages = {1--23},
title = {{Probability theory}},
volume = {941},
year = {2017}
}
@article{Kim2021,
abstract = {In this paper, we investigate local permutation tests for testing conditional independence between two random vectors $X$ and $Y$ given $Z$. The local permutation test determines the significance of a test statistic by locally shuffling samples which share similar values of the conditioning variables $Z$, and it forms a natural extension of the usual permutation approach for unconditional independence testing. Despite its simplicity and empirical support, the theoretical underpinnings of the local permutation test remain unclear. Motivated by this gap, this paper aims to establish theoretical foundations of local permutation tests with a particular focus on binning-based statistics. We start by revisiting the hardness of conditional independence testing and provide an upper bound for the power of any valid conditional independence test, which holds when the probability of observing collisions in $Z$ is small. This negative result naturally motivates us to impose additional restrictions on the possible distributions under the null and alternate. To this end, we focus our attention on certain classes of smooth distributions and identify provably tight conditions under which the local permutation method is universally valid, i.e. it is valid when applied to any (binning-based) test statistic. To complement this result on type I error control, we also show that in some cases, a binning-based statistic calibrated via the local permutation method can achieve minimax optimal power. We also introduce a double-binning permutation strategy, which yields a valid test over less smooth null distributions than the typical single-binning method without compromising much power. Finally, we present simulation results to support our theoretical findings.},
archivePrefix = {arXiv},
arxivId = {2112.11666},
author = {Kim, Ilmun and Neykov, Matey and Balakrishnan, Sivaraman and Wasserman, Larry},
eprint = {2112.11666},
file = {::},
journal = {Annals of Statistics},
keywords = {conditional independence testing},
mendeley-tags = {conditional independence testing},
number = {6},
pages = {3388--3414},
title = {{Local permutation tests for conditional independence}},
url = {http://arxiv.org/abs/2112.11666},
volume = {50},
year = {2022}
}
@article{SetS19,
annote = {\href{https://www.biorxiv.org/content/10.1101/631390v2}{https://www.biorxiv.org/content/10.1101/631390v2}},
author = {Sesia, Matteo and Katsevich, Eugene and Bates, Stephen and Cand{\`{e}}s, Emmanuel and Sabatti, Chiara},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sesia et al. - 2020 - Multi-resolution localization of causal variants across the genome(3).pdf:pdf},
journal = {Nature Communications},
keywords = {GWAS,groups,knockoffs,model-X,unpublished},
mendeley-tags = {GWAS,groups,knockoffs,model-X,unpublished},
pages = {1093},
title = {{Multi-resolution localization of causal variants across the genome}},
volume = {11},
year = {2020}
}
@article{Bickel2006,
abstract = {We introduce a new framework for constructing tests of general semi-parametric hypotheses which have nontrivial power on the n- 1/2 scale in every direction, and can be tailored to put substantial power on alternatives of importance. The approach is based on combining test statistics based on stochastic processes of score statistics with bootstrap critical values. {\textcopyright} Institute of Mathematical Statistics, 2006.},
author = {Bickel, Peter J. and Ritov, Ya'acov and Stoker, Thomas M.},
doi = {10.1214/009053606000000137},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Copula models,Independence,Mixture of Gaussians},
number = {2},
pages = {721--741},
title = {{Tailor-made tests for goodness of fit to semiparametric hypotheses}},
volume = {34},
year = {2006}
}
@article{Westling2021,
abstract = {In many scientific studies, it is of interest to determine whether an exposure has a causal effect on an outcome. In observational studies, this is a challenging task due to the presence of confounding variables that affect both the exposure and the outcome. Many methods have been developed to test for the presence of a causal effect when all such confounding variables are observed and when the exposure of interest is discrete. In this article, we propose a class of nonparametric tests of the null hypothesis that there is no average causal effect of an arbitrary univariate exposure on an outcome in the presence of observed confounding. Our tests apply to discrete, continuous, and mixed discrete-continuous exposures. We demonstrate that our proposed tests are doubly robust consistent, that they have correct asymptotic Type I error if both nuisance parameters involved in the problem are estimated at fast enough rates, and that they have power to detect local alternatives approaching the null at the rate (Formula presented.). We study the performance of our tests in numerical studies, and use them to test for the presence of a causal effect of BMI on immune response in early phase vaccine trials. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {2001.05344},
author = {Westling, Ted},
doi = {10.1080/01621459.2020.1865168},
eprint = {2001.05344},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Continuous exposure,Dose-response,G-computation,Observational study,Uniform inference},
number = {0},
pages = {1--26},
publisher = {Taylor & Francis},
title = {{Nonparametric Tests of the Causal Null With Nondiscrete Exposures}},
url = {https://doi.org/10.1080/01621459.2020.1865168},
volume = {0},
year = {2021}
}
@article{Shah2022,
archivePrefix = {arXiv},
arxivId = {arXiv:1909.10828v2},
author = {Shah, Rajen D and Buhlmann, Peter},
eprint = {arXiv:1909.10828v2},
file = {::},
journal = {Statistical Science},
keywords = {double robustness,high-dimensional regression},
mendeley-tags = {double robustness,high-dimensional regression},
title = {{Double-estimation-friendly inference for high-dimensional misspecified models}},
year = {2022}
}
@article{TchetgenTchetgen2010,
abstract = {We consider the doubly robust estimation of the parameters in a semiparametric conditional odds ratio model. Our estimators are consistent and asymptotically normal in a union model that assumes either of two variation independent baseline functions is correctly modelled but not necessarily both. Furthermore, when either outcome has finite support, our estimators are semiparametric efficient in the union model at the intersection submodel where both nuisance functions models are correct. For general outcomes, we obtain doubly robust estimators that are nearly efficient at the intersection submodel. Our methods are easy to implement as they do not require the use of the alternating conditional expectations algorithm of Chen (2007). 2009 Biometrika Trust.},
author = {{Tchetgen Tchetgen}, Eric J. and Robins, James M. and Rotnitzky, Andrea},
doi = {10.1093/biomet/asp062},
file = {::},
issn = {00063444},
journal = {Biometrika},
keywords = {Doubly robust,Generalized odds ratio,Locally efficient,Semiparametric logistic regression,double robustness,generalized linear models},
mendeley-tags = {double robustness,generalized linear models},
number = {1},
pages = {171--180},
title = {{On doubly robust estimation in a semiparametric odds ratio model}},
volume = {97},
year = {2010}
}
@article{Sesia2021,
abstract = {We present a comprehensive statistical framework to analyze data from genome-wide association studies of polygenic traits, producing interpretable findings while controlling the false discovery rate. In contrast with standard approaches, our method can leverage sophisticated multivariate algorithms but makes no parametric assumptions about the unknown relation between genotypes and phenotype. Instead, we recognize that genotypes can be considered as a random sample from an appropriate model, encapsulating our knowledge of genetic inheritance and human populations. This allows the generation of imperfect copies (knockoffs) of these variables that serve as ideal negative controls, correcting for linkage disequilibrium and accounting for unknown population structure, which may be due to diverse ancestries or familial relatedness. The validity and effectiveness of our method are demonstrated by extensive simulations and by applications to the UK Biobank data. These analyses confirm our method is powerful relative to state-of-the-art alternatives, while comparisons with other studies validate most of our discoveries. Finally, fast software is made available for researchers to analyze Biobank-scale datasets.},
author = {Sesia, Matteo and Bates, Stephen and Cand{\`{e}}s, Emmanuel and Marchini, Jonathan and Sabatti, Chiara},
doi = {10.1073/pnas.2105841118},
file = {::},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {FDR,False discovery rate,GWAS,Genome-wide association studies,Hidden Markov models,Knockoffs,Population structure,knockoffs,model-X},
mendeley-tags = {FDR,GWAS,knockoffs,model-X},
number = {40},
pages = {1--12},
pmid = {34580220},
title = {{False discovery rate control in genome-wide association studies with population structure}},
volume = {118},
year = {2021}
}
@article{Robins1992,
author = {Robins, James M. and Mark, Steven D. and Newey, Whitney K.},
file = {::},
journal = {Biometrics},
keywords = {causality},
mendeley-tags = {causality},
number = {2},
pages = {479--495},
title = {{Estimating Exposure Effects by Modelling the Expectation of Exposure Conditional on Confounders}},
volume = {48},
year = {1992}
}
@article{PrakasaRao2009,
abstract = {Some properties of conditionally independent random variables are studied. Conditional versions of generalized Borel-Cantelli lemma, generalized Kolmogorov's inequality and generalized H{\'{a}}jek-R{\'{e}}nyi inequality are proved. As applications, a conditional version of the strong law of large numbers for conditionally independent random variables and a conditional version of the Kolmogorov's strong law of large numbers for conditionally independent random variables with identical conditional distributions are obtained. The notions of conditional strong mixing and conditional association for a sequence of random variables are introduced. Some covariance inequalities and a central limit theorem for such sequences are mentioned. {\textcopyright} 2007 The Institute of Statistical Mathematics, Tokyo.},
author = {{Prakasa Rao}, B. L.S.},
doi = {10.1007/s10463-007-0152-2},
file = {::},
issn = {00203157},
journal = {Annals of the Institute of Statistical Mathematics},
keywords = {Conditional Borel-Cantelli lemma,Conditional H{\'{a}}jek-R{\'{e}}nyi inequality,Conditional association,Conditional independence,Conditional mixing,Conditional strong law of large numbers,Generalized Kolmogorov inequality},
number = {2},
pages = {441--460},
title = {{Conditional independence, conditional mixing and conditional association}},
volume = {61},
year = {2009}
}
@article{Tan2019,
abstract = {Consider a logistic partially linear model, in which the logit of the mean of a binary response is related to a linear function of some covariates and a nonparametric function of other covariates. We derive simple, doubly robust estimators of coefficients in the linear component. Such estimators remain consistent if either a nuisance model is correctly specified for the nonparametric component, or another nuisance model is correctly specified for the means of the covariates of interest given other covariates and the response at a fixed value.},
archivePrefix = {arXiv},
arxivId = {1901.09138},
author = {Tan, Zhiqiang},
doi = {10.1016/j.spl.2019.108577},
eprint = {1901.09138},
file = {::},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Double robustness,Local efficiency,Logistic models,Odds ratio,Partially linear models,Semiparametric models,double robustness,generalized linear models,logistic regression},
mendeley-tags = {double robustness,generalized linear models,logistic regression},
pages = {108577},
publisher = {Elsevier B.V.},
title = {{On doubly robust estimation for logistic partially linear models}},
url = {https://doi.org/10.1016/j.spl.2019.108577},
volume = {155},
year = {2019}
}
@article{Canonne2018,
abstract = {We study the problem of testing conditional independence for discrete distributions. Specifically, given samples from a discrete random variable (X, Y, Z) on domain [ℓ1]×[ℓ2]×[n], we want to distinguish, with probability at least 2/3, between the case that X and Y are conditionally independent given Z from the case that (X, Y, Z) is ϵ-far, in ℓ1-distance, from every distribution that has this property. Conditional independence is a concept of central importance in probability and statistics with a range of applications in various scientific domains. As such, the statistical task of testing conditional independence has been extensively studied in various forms within the statistics and econometrics communities for nearly a century. Perhaps surprisingly, this problem has not been previously considered in the framework of distribution property testing and in particular no tester with sublinear sample complexity is known, even for the important special case that the domains of X and Y are binary. The main algorithmic result of this work is the first conditional independence tester with sublinear sample complexity for discrete distributions over [ℓ1]×[ℓ2]×[n]. To complement our upper bounds, we prove information-theoretic lower bounds establishing that the sample complexity of our algorithm is optimal, up to constant factors, for a number of settings. Specifically, for the prototypical setting when ℓ1,ℓ2=O(1), we show that the sample complexity of testing conditional independence (upper bound and matching lower bound) is (equation presented) To obtain our tester, we employ a variety of tools, including (1) a suitable weighted adaptation of the flattening technique [DK16], and (2) the design and analysis of an optimal (unbiased) estimator for the following statistical problem of independent interest: Given a degree -d polynomial Q:Rn→R and sample access to a distribution p over [n], estimate Q(p1,⋯, pn) up to small additive error. Obtaining tight variance analyses for specific estimators of this form has been a major technical hurdle in distribution testing (see, e.g., [CDVV14]). As an important contribution of this work, we develop a general theory providing tight variance bounds for all such estimators. Our lower bounds, established using the mutual information method, rely on novel constructions of hard instances that may be useful in other settings.},
author = {Canonne, Clement L. and Diakonikolas, Ilias and Kane, Daniel M. and Stewart, Alistair},
doi = {10.1109/ITA.2018.8503255},
file = {::},
isbn = {9781728101248},
journal = {2018 Information Theory and Applications Workshop, ITA 2018},
keywords = {con-,discrete distributions,distribution testing,ditional independence,probability distributions,property testing,sublinear algorithms},
pages = {735--748},
title = {{Testing conditional independence of discrete distributions}},
year = {2018}
}
@article{Bach2021,
abstract = {DoubleML is an open-source Python library implementing the double machine learning framework of Chernozhukov et al. (2018) for a variety of causal models. It contains functionalities for valid statistical inference on causal parameters when the estimation of nuisance parameters is based on machine learning methods. The object-oriented implementation of DoubleML provides a high flexibility in terms of model specifications and makes it easily extendable. The package is distributed under the MIT license and relies on core libraries from the scientific Python ecosystem: scikit-learn, numpy, pandas, scipy, statsmodels and joblib. Source code, documentation and an extensive user guide can be found at https://github.com/DoubleML/doubleml-for-py and https://docs.doubleml.org.},
archivePrefix = {arXiv},
arxivId = {2104.03220},
author = {Bach, Philipp and Chernozhukov, Victor and Kurz, Malte S. and Spindler, Martin},
eprint = {2104.03220},
file = {::},
keywords = {causal inference,causal machine learning,machine learning,mlr3,object orientation,r},
pages = {1--42},
title = {{DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R}},
url = {http://arxiv.org/abs/2104.03220},
year = {2021}
}
@article{Aufiero2022,
abstract = {Computational models are utilized in many scientific domains to simulate complex systems. Sensitivity analysis is an important practice to aid our understanding of the mechanics of these models and the processes they describe, but performing a sufficient number of model evaluations to obtain accurate sensitivity estimates can often be prohibitively expensive. In order to reduce the computational burden, a common solution is to use a surrogate model that approximates the original model reasonably well but at a fraction of the cost. However, in exchange for the computational benefits of surrogate-based sensitivity analysis, this approach comes with the price of a loss in accuracy arising from the difference between the surrogate and the original model. To address this issue, we adapt the floodgate method of Zhang and Janson (2020) to provide valid surrogate-based confidence intervals rather than a point estimate, allowing for the benefit of the computational speed-up of using a surrogate that is especially pronounced for high-dimensional models, while still retaining rigorous and accurate bounds on the global sensitivity with respect to the original (non-surrogate) model. Our confidence interval is asymptotically valid with almost no conditions on the computational model or the surrogate. Additionally, the accuracy (width) of our confidence interval shrinks as the surrogate's accuracy increases, so when an accurate surrogate is used, the confidence interval we report will correspondingly be quite narrow, instilling appropriately high confidence in its estimate. We demonstrate the properties of our method through numerical simulations on the small Hymod hydrological model, and also apply it to the more complex CBM-Z meteorological model with a recent neural-network-based surrogate.},
author = {Aufiero, Massimo and Janson, Lucas},
file = {::},
journal = {arXiv},
keywords = {model-X},
mendeley-tags = {model-X},
title = {{Surrogate-based global sensitivity analysis with statistical guarantees via floodgate}},
year = {2022}
}
@article{Javanmard2018a,
abstract = {Performing statistical inference in high-dimensional models is challenging because of the lack of precise information on the distribution of high-dimensional regularized estimators. Here, we consider linear regression in the high-dimensional regime p n and the Lasso estimator: we would like to perform inference on the parameter vector $\theta$∗ ∈ Rp. Important progress has been achieved in computing confidence intervals and p-values for single coordinates $\theta$i∗, i ∈ {1, . . ., p}. A key role in these new inferential methods is played by a certain debiased estimator $\theta$d. Earlier work establishes that, under suitable assumptions on the design matrix, the coordinates of $\theta$d are asymptotically Gaussian provided the true parameters vector $\theta$∗ is s0-sparse with s0 = o(n/log p). The condition s0 = o(n/log p) is considerably stronger than the one for consistent estimation, namely s0 = o(n/log p). In this paper, we consider Gaussian designs with known or unknown population covariance. When the covariance is known, we prove that the debiased estimator is asymptotically Gaussian under the nearly optimal condition s0 = o(n/(log p)2). The same conclusion holds if the population covariance is unknown but can be estimated sufficiently well. For intermediate regimes, we describe the trade-off between sparsity in the coefficients $\theta$∗, and sparsity in the inverse covariance of the design. We further discuss several applications of our results beyond high-dimensional inference. In particular, we propose a thresholded Lasso estimator that is minimax optimal up to a factor 1 + on(1) for i.i.d. Gaussian designs.},
archivePrefix = {arXiv},
arxivId = {1508.02757},
author = {Javanmard, Adel and Montanari, Andrea},
doi = {10.1214/17-AOS1630},
eprint = {1508.02757},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bias,Confidence intervals,High-dimensional regression,Hypothesis testing,Lasso,Sample size,Variance},
number = {6A},
pages = {2593--2622},
title = {{Debiasing the lasso: Optimal sample size for Gaussian designs}},
volume = {46},
year = {2018}
}
@article{Liu2020,
author = {Liu, Molei and Katsevich, Eugene and Ramdas, Aaditya and Janson, Lucas},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2020 - Fast and Powerful Conditional Randomization Testing via Distillation(2).pdf:pdf},
journal = {Biometrika},
keywords = {conditional independence testing,conditional randomization test,crt,high-dimensional inference,high-dimensional regression,machine learning,model-X,model-x},
mendeley-tags = {conditional randomization test,high-dimensional regression,model-X},
title = {{Fast and Powerful Conditional Randomization Testing via Distillation}},
url = {https://arxiv.org/abs/2006.03980},
year = {2021}
}
@article{Spector2022a,
author = {Spector, Asher and Fithian, William},
file = {::},
journal = {arXiv},
keywords = {knockoffs},
mendeley-tags = {knockoffs},
title = {{Asymptotically Optimal Knockoff Statistics via the Masked Likelihood Ratio}},
year = {2022}
}
@article{Bates2020a,
abstract = {This paper proposes a novel statistical method to address population structure in genome-wide association studies while controlling the false discovery rate, which overcomes some limitations of existing approaches. Our solution accounts for linkage disequilibrium and diverse ancestries by combining conditional testing via knockoffs with hidden Markov models from state-of-the-art phasing methods. Furthermore, we account for familial relatedness by describing the joint distribution of haplotypes sharing long identical-by-descent segments with a generalized hidden Markov model. Extensive simulations affirm the validity of this method, while applications to UK Biobank phenotypes yield many more discoveries compared to BOLT-LMM, most of which are confirmed by the Japan Biobank and FinnGen data.},
author = {Bates, Stephen and Cand{\`{e}}s, Emmanuel and Marchini, Jonathan and Sabatti, Chiara},
doi = {10.1101/2020.08.04.236703},
file = {::},
journal = {Proceedings of the National Academy of Sciences},
keywords = {GWAS,knockoffs},
mendeley-tags = {GWAS,knockoffs},
number = {40},
title = {{Controlling the false discovery rate in GWAS with population structure}},
url = {http://biorxiv.org/cgi/content/short/2020.08.04.236703v1?rss=1&utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound},
volume = {118},
year = {2021}
}
@article{Chung2013,
abstract = {Given independent samples from P andQ, two-sample permutation tests allow one to construct exact level tests when the null hypothesis is P = Q. On the other hand, when comparing or testing particular parameters $\theta$ of P and Q, such as their means or medians, permutation tests need not be level $\alpha$, or even approximately level $\alpha$ in large samples. Under very weak assumptions for comparing estimators, we provide a general test procedure whereby the asymptotic validity of the permutation test holds while retaining the exact rejection probability $\alpha$ in finite samples when the underlying distributions are identical. The ideas are broadly applicable and special attention is given to the k-sample problem of comparing general parameters, whereby a permutation test is constructed which is exact level $\alpha$ under the hypothesis of identical distributions, but has asymptotic rejection probability $\alpha$ under the more general null hypothesis of equality of parameters. A Monte Carlo simulation study is performed as well. A quite general theory is possible based on a coupling construction, as well as a key contiguity argument for the multinomial and multivariate hypergeometric distributions. {\textcopyright} Institute of Mathematical Statistics, 2013.},
author = {Chung, Eunyi and Romano, Joseph P.},
doi = {10.1214/13-AOS1090},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Behrens-Fisher problem,Coupling,Permutation test},
number = {2},
pages = {484--507},
title = {{Exact and asymptotically robust permutation tests}},
volume = {41},
year = {2013}
}
@article{Robins2007,
abstract = {When outcomes are missing for reasons beyond an investigator's control, there are two different ways to adjust a parameter estimate for covariates that may be related both to the outcome and to missingness. One approach is to model the relationships between the covariates and the outcome and use those relationships to predict the missing values. Another is to model the probabilities of missingness given the covariates and incorporate them into a weighted or stratified estimate. Doubly robust (DR) procedures apply both types of model simultaneously and produce a consistent estimate of the parameter if either of the two models has been correctly specified. In this article, we show that DR estimates can be constructed in many ways. We compare the performance of various DR and non-DR estimates of a population mean in a simulated example where both models are incorrect but neither is grossly misspecified. Methods that use inverse-probabilities as weights, whether they are DR or not, are sensitive to misspecification of the propensity model when some estimated propensities are small. Many DR methods perform better than simple inverse-probability weighting. None of the DR methods we tried, however, improved upon the performance of simple regression-based prediction of the missing values. This study does not represent every missing-data problem that will arise in practice. But it does demonstrate that, in at least some settings, two wrong models are not better than one. {\textcopyright} Institute of Mathematical Statistics, 2007.},
archivePrefix = {arXiv},
arxivId = {arXiv:0804.2965v1},
author = {Robins, James and Sued, Mariela and Lei-Gomez, Quanhong and Rotnitzky, Andrea},
doi = {10.1214/07-STS227},
eprint = {arXiv:0804.2965v1},
file = {::},
issn = {08834237},
journal = {Statistical Science},
keywords = {Causal inference,Missing data,Model-assisted survey estimation,Propensity score,Weighted estimating equations},
number = {4},
pages = {544--559},
pmid = {18516239},
title = {{Comment: Performance of Double-Robust Estimators When “Inverse Probability” Weights Are Highly Variable}},
volume = {22},
year = {2007}
}
@article{Li2020b,
abstract = {Testing conditional independence (CI) for continuous variables is a fundamental but challenging task in statistics. Many tests for this task are developed and used increasingly widely by data analysts. This article reviews the current status of the nonparametric part of these tests, which assumes no parametric form for the joint continuous density function. The different ways to approach the CI are summarized. Tests are also grouped according to their data assumptions and method types. A numerical comparison is also conducted for representative tests. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Analysis of High Dimensional Data Statistical and Graphical Methods of Data Analysis > Multivariate Analysis.},
author = {Li, Chun and Fan, Xiaodan},
doi = {10.1002/wics.1489},
file = {:C\:/Users/abch/Downloads/wics.1489.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {conditional independence,hypothesis testing,literature review},
number = {3},
pages = {1--11},
title = {{On nonparametric conditional independence tests for continuous variables}},
volume = {12},
year = {2020}
}
@article{ZZ14,
author = {Zhang, Cun-Hui and Zhang, Stephanie S},
file = {::},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {1},
pages = {217--242},
publisher = {Wiley Online Library},
title = {{Confidence intervals for low dimensional parameters in high dimensional linear models}},
volume = {76},
year = {2014}
}
@article{Robinson1988,
author = {Robinson, P. M.},
file = {::},
journal = {Econometrica},
keywords = {causality},
mendeley-tags = {causality},
number = {4},
pages = {931--954},
title = {{Root-N-Consistent Semiparametric Regression}},
volume = {56},
year = {1988}
}
@article{Vansteelandt2020,
abstract = {Inference for the parameters indexing generalised linear models is routinely based on the assumption that the model is correct and a priori specified. This is unsatisfactory because the chosen model is usually the result of a data-adaptive model selection process, which may induce excess uncertainty that is not usually acknowledged. Moreover, the assumptions encoded in the chosen model rarely represent some a priori known, ground truth, making standard inferences prone to bias, but also failing to give a pure reflection of the information that is contained in the data. Inspired by developments on assumption-free inference for so-called projection parameters, we here propose novel nonparametric definitions of main effect estimands and effect modification estimands. These reduce to standard main effect and effect modification parameters in generalised linear models when these models are correctly specified, but have the advantage that they continue to capture respectively the primary (conditional) association between two variables, or the degree to which two variables interact (in a statistical sense) in their effect on outcome, even when these models are misspecified. We achieve an assumption-lean inference for these estimands (and thus for the underlying regression parameters) by deriving their influence curve under the nonparametric model and invoking flexible data-adaptive (e.g., machine learning) procedures.},
archivePrefix = {arXiv},
arxivId = {2006.08402},
author = {Vansteelandt, Stijn and Dukes, Oliver},
eprint = {2006.08402},
file = {::},
journal = {arXiv},
keywords = {bias,conditional treatment effect,double robustness,estimand,generalized linear models,influence curve,interaction,model misspecification,nonparametric inference},
mendeley-tags = {double robustness,generalized linear models},
title = {{Assumption-lean inference for generalised linear model parameters}},
url = {http://arxiv.org/abs/2006.08402},
year = {2020}
}
@book{Hardle2000,
author = {H{\"{a}}rdle, Wolfgang and Liang, Hua and Gao, Jiti},
publisher = {Springer Science \& Business Media},
title = {{Partially linear models}},
year = {2000}
}
@article{Barber2018,
abstract = {We consider the variable selection problem, which seeks to identify important variables influencing a response $Y$ out of many candidate features $X_1, \ldots, X_p$. We wish to do so while offering finite-sample guarantees about the fraction of false positives - selected variables $X_j$ that in fact have no effect on $Y$ after the other features are known. When the number of features $p$ is large (perhaps even larger than the sample size $n$), and we have no prior knowledge regarding the type of dependence between $Y$ and $X$, the model-X knockoffs framework nonetheless allows us to select a model with a guaranteed bound on the false discovery rate, as long as the distribution of the feature vector $X=(X_1,\dots,X_p)$ is exactly known. This model selection procedure operates by constructing "knockoff copies'" of each of the $p$ features, which are then used as a control group to ensure that the model selection algorithm is not choosing too many irrelevant features. In this work, we study the practical setting where the distribution of $X$ could only be estimated, rather than known exactly, and the knockoff copies of the $X_j$'s are therefore constructed somewhat incorrectly. Our results, which are free of any modeling assumption whatsoever, show that the resulting model selection procedure incurs an inflation of the false discovery rate that is proportional to our errors in estimating the distribution of each feature $X_j$ conditional on the remaining features $\{X_k:k\neq j\}$. The model-X knockoff framework is therefore robust to errors in the underlying assumptions on the distribution of $X$, making it an effective method for many practical applications, such as genome-wide association studies, where the underlying distribution on the features $X_1,\dots,X_p$ is estimated accurately but not known exactly.},
archivePrefix = {arXiv},
arxivId = {1801.03896},
author = {Barber, Rina Foygel and Cand{\`{e}}s, Emmanuel J. and Samworth, Richard J.},
eprint = {1801.03896},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barber, Cand{\`{e}}s, Samworth - 2020 - Robust inference with knockoffs.pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics,},
number = {3},
pages = {1409--1431},
title = {{Robust inference with knockoffs}},
url = {http://arxiv.org/abs/1801.03896},
volume = {48},
year = {2020}
}
@article{Huang2019,
abstract = {The recent paper Cand\`es et al. (2018) introduced model-X knockoffs, a method for variable selection that provably and non-asymptotically controls the false discovery rate with no restrictions or assumptions on the dimensionality of the data or the conditional distribution of the response given the covariates. The one requirement for the procedure is that the covariate samples are drawn independently and identically from a precisely-known (but arbitrary) distribution. The present paper shows that the exact same guarantees can be made without knowing the covariate distribution fully, but instead knowing it only up to a parametric model with as many as $\Omega(n^{*}p)$ parameters, where $p$ is the dimension and $n^{*}$ is the number of covariate samples (which may exceed the usual sample size $n$ of labeled samples when unlabeled samples are also available). The key is to treat the covariates as if they are drawn conditionally on their observed value for a sufficient statistic of the model. Although this idea is simple, even in Gaussian models conditioning on a sufficient statistic leads to a distribution supported on a set of zero Lebesgue measure, requiring techniques from topological measure theory to establish valid algorithms. We demonstrate how to do this for three models of interest, with simulations showing the new approach remains powerful under the weaker assumptions.},
author = {Huang, Dongming and Janson, Lucas},
file = {::},
journal = {Annals of Statistics},
keywords = {ery rate,false discov-,fdr,graphical model,high-dimensional inference,knockoffs,model-x,sufficient statistic,topological measure},
mendeley-tags = {knockoffs},
number = {5},
pages = {3021--3042},
title = {{Relaxing the Assumptions of Knockoffs by Conditioning}},
url = {http://arxiv.org/abs/1903.02806},
volume = {48},
year = {2020}
}
@article{Shekhar2022,
abstract = {In nonparametric independence testing, we observe i.i.d.\ data $\{(X_i,Y_i)\}_{i=1}^n$, where $X \in \mathcal{X}, Y \in \mathcal{Y}$ lie in any general spaces, and we wish to test the null that $X$ is independent of $Y$. Modern test statistics such as the kernel Hilbert-Schmidt Independence Criterion (HSIC) and Distance Covariance (dCov) have intractable null distributions due to the degeneracy of the underlying U-statistics. Thus, in practice, one often resorts to using permutation testing, which provides a nonasymptotic guarantee at the expense of recalculating the quadratic-time statistics (say) a few hundred times. This paper provides a simple but nontrivial modification of HSIC and dCov (called xHSIC and xdCov, pronounced ``cross'' HSIC/dCov) so that they have a limiting Gaussian distribution under the null, and thus do not require permutations. This requires building on the newly developed theory of cross U-statistics by Kim and Ramdas (2020), and in particular developing several nontrivial extensions of the theory in Shekhar et al. (2022), which developed an analogous permutation-free kernel two-sample test. We show that our new tests, like the originals, are consistent against fixed alternatives, and minimax rate optimal against smooth local alternatives. Numerical simulations demonstrate that compared to the full dCov or HSIC, our variants have the same power up to a $\sqrt 2$ factor, giving practitioners a new option for large problems or data-analysis pipelines where computation, not sample size, could be the bottleneck.},
archivePrefix = {arXiv},
arxivId = {2212.09108},
author = {Shekhar, Shubhanshu and Kim, Ilmun and Ramdas, Aaditya},
eprint = {2212.09108},
file = {::},
journal = {Journal of Machine Learning Research},
number = {369},
pages = {1--68},
title = {{A Permutation-Free Kernel Independence Test}},
url = {http://arxiv.org/abs/2212.09108},
volume = {24},
year = {2023}
}
@article{Celentano2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2007.13716v1},
author = {Celentano, Michael and Montanari, Andrea and Wei, Yuting},
eprint = {arXiv:2007.13716v1},
file = {::},
journal = {arXiv},
keywords = {AMP,Lasso,conditional randomization test,high-dimensional regression},
mendeley-tags = {AMP,Lasso,conditional randomization test,high-dimensional regression},
title = {{The Lasso with general Gaussian designs with applications to hypothesis testing}},
year = {2020}
}
@inproceedings{Liu2019,
abstract = {The knockoff filter introduced by Barber and Cand\`es 2016 is an elegant framework for controlling the false discovery rate in variable selection. While empirical results indicate that this methodology is not too conservative, there is no conclusive theoretical result on its power. When the predictors are i.i.d. Gaussian, it is known that as the signal to noise ratio tend to infinity, the knockoff filter is consistent in the sense that one can make FDR go to 0 and power go to 1 simultaneously. In this work we study the case where the predictors have a general covariance matrix $\Sigma$. We introduce a simple functional called effective signal deficiency (ESD) of the covariance matrix $\Sigma$ that predicts consistency of various variable selection methods. In particular, ESD reveals that the structure of the precision matrix $\Sigma^{-1}$ plays a central role in consistency and therefore, so does the conditional independence structure of the predictors. To leverage this connection, we introduce Conditional Independence knockoff, a simple procedure that is able to compete with the more sophisticated knockoff filters and that is defined when the predictors obey a Gaussian tree graphical models (or when the graph is sufficiently sparse). Our theoretical results are supported by numerical evidence on synthetic data.},
archivePrefix = {arXiv},
arxivId = {1910.12428},
author = {Liu, Jingbo and Rigollet, Philippe},
booktitle = {33rd Conference on Neural Information Processing Systems},
eprint = {1910.12428},
file = {::},
title = {{Power analysis of knockoff filters for correlated designs}},
url = {http://arxiv.org/abs/1910.12428},
year = {2019}
}
@book{Kosorok2008,
address = {New York},
author = {Kosorok, Michael R.},
file = {::},
issn = {03067734},
publisher = {Springer},
title = {{Introduction to Empirical Processes and Semiparametric Inference}},
year = {2008}
}
@article{Shah2018,
abstract = {It is a common saying that testing for conditional independence, i.e., testing whether whether two random vectors $X$ and $Y$ are independent, given $Z$, is a hard statistical problem if $Z$ is a continuous random variable (or vector). In this paper, we prove that conditional independence is indeed a particularly difficult hypothesis to test for. Valid statistical tests are required to have a size that is smaller than a predefined significance level, and different tests usually have power against a different class of alternatives. We prove that a valid test for conditional independence does not have power against any alternative. Given the non-existence of a uniformly valid conditional independence test, we argue that tests must be designed so their suitability for a particular problem may be judged easily. To address this need, we propose in the case where $X$ and $Y$ are univariate to nonlinearly regress $X$ on $Z$, and $Y$ on $Z$ and then compute a test statistic based on the sample covariance between the residuals, which we call the generalised covariance measure (GCM). We prove that validity of this form of test relies almost entirely on the weak requirement that the regression procedures are able to estimate the conditional means $X$ given $Z$, and $Y$ given $Z$, at a slow rate. We extend the methodology to handle settings where $X$ and $Y$ may be multivariate or even high-dimensional. While our general procedure can be tailored to the setting at hand by combining it with any regression technique, we develop the theoretical guarantees for kernel ridge regression. A simulation study shows that the test based on GCM is competitive with state of the art conditional independence tests. Code is available as the R package GeneralisedCovarianceMeasure on CRAN.},
archivePrefix = {arXiv},
arxivId = {1804.07203},
author = {Shah, Rajen D. and Peters, Jonas},
eprint = {1804.07203},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shah, Peters - 2020 - The Hardness of Conditional Independence Testing and the Generalised Covariance Measure(2).pdf:pdf},
issn = {0090-5364},
journal = {Annals of Statistics},
keywords = {causality,conditional randomization test},
mendeley-tags = {causality,conditional randomization test},
number = {3},
pages = {1514--1538},
title = {{The Hardness of Conditional Independence Testing and the Generalised Covariance Measure}},
volume = {48},
year = {2020}
}
@book{Strasser1985,
address = {Berlin},
author = {Strasser, Helmut},
file = {::},
isbn = {0899250289},
publisher = {de Gruyter},
title = {{Mathematical Theory of Statistics}},
year = {1985}
}
@article{Bates2020,
archivePrefix = {arXiv},
arxivId = {arXiv:2002.09644v1},
author = {Bates, Stephen and Sesia, Matteo and Sabatti, Chiara and Candes, Emmanuel},
eprint = {arXiv:2002.09644v1},
file = {::},
journal = {Proceedings of the National Academy of Sciences},
keywords = {causal discovery,conditional independence,false discovery rate,family-based association test,fbat,fdr,genome-,gwas,tdt,transmission disequilibrium test,trio,wide association study},
number = {39},
pages = {24117--24126},
title = {{Causal Inference in Genetic Trio Studies}},
volume = {117},
year = {2020}
}
@article{Jankova2020,
author = {Jankova, Jana and Shah, Rajen D and Buhlmann, Peter},
doi = {10.1111/rssb.12371},
file = {::},
journal = {Journal of the Royal Statistical Society, Series B},
keywords = {debiasing,generalized linear models,goodness-of-fit testing,group testing,high dimensional data,high-dimensional regression,residual prediction},
mendeley-tags = {high-dimensional regression},
title = {{Goodness-of-fit testing in high dimensional generalized linear models}},
year = {2020}
}
@article{Dedecker2002,
abstract = {The arrays with rows consisting of conditionally independent random variables with respect to certain $\sigma$-algebras are studied. An analogue of the Lindeberg–Feller theorem known for systems of independent random variables is established. This result is based on the theorem proved by Yuan, Wei, and Lei in [J. Korean Math. Soc., 51 (2014), pp. 1–15], where the authors considered a sequence of random variables conditionally independent with respect to a given $\sigma$-algebra. They were interested in a.s. convergence, whereas our version of the Lindeberg condition in a weak form (involving convergence in probability) is less restrictive. An application of the indicated new result for arrays provides an extension of conditions for asymptotic normality of the estimates of the regression function second moment obtained in a recent paper by Gy{\"{o}}rfi and Walk [J. Mach. Learn. Res., 16 (2015), pp. 1863-1877].},
author = {Dedecker, Jerome and Merlevede, Florence},
doi = {10.1137/S0040585X97T98837X},
file = {::},
issn = {10957219},
journal = {Annals of Probability},
keywords = {Array of random variables,Conditional central limit theorem,Conditional characteristic functions,Conditional independence,Feature selection,Regression function moments},
number = {3},
pages = {1044--1081},
title = {{Necessary and sufficient conditions for the conditional central limit theorem}},
volume = {30},
year = {2002}
}
@article{BetH14,
author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belloni, Chernozhukov, Hansen - 2014 - Inference on treatment effects after selection among high-dimensional controls.pdf:pdf},
journal = {The Review of Economic Studies},
number = {2},
pages = {608--650},
publisher = {Oxford University Press},
title = {{Inference on treatment effects after selection among high-dimensional controls}},
volume = {81},
year = {2014}
}
@article{Zhao2020,
abstract = {We study the distribution of the maximum likelihood estimate (MLE) in high-dimensional logistic models, extending the recent results from Sur (2019) to the case where the Gaussian covariates may have an arbitrary covariance structure. We prove that in the limit of large problems holding the ratio between the number $p$ of covariates and the sample size $n$ constant, every finite list of MLE coordinates follows a multivariate normal distribution. Concretely, the $j$th coordinate $\hat {\beta}_j$ of the MLE is asymptotically normally distributed with mean $\alpha_\star \beta_j$ and standard deviation $\sigma_\star/\tau_j$; here, $\beta_j$ is the value of the true regression coefficient, and $\tau_j$ the standard deviation of the $j$th predictor conditional on all the others. The numerical parameters $\alpha_\star > 1$ and $\sigma_\star$ only depend upon the problem dimensionality $p/n$ and the overall signal strength, and can be accurately estimated. Our results imply that the MLE's magnitude is biased upwards and that the MLE's standard deviation is greater than that predicted by classical theory. We present a series of experiments on simulated and real data showing excellent agreement with the theory.},
archivePrefix = {arXiv},
arxivId = {2001.09351},
author = {Zhao, Qian and Sur, Pragya and Cand{\`{e}}s, Emmanuel J.},
eprint = {2001.09351},
file = {::},
journal = {arXiv},
keywords = {AMP,high-dimensional regression,logistic regression},
mendeley-tags = {AMP,high-dimensional regression,logistic regression},
pages = {1--29},
title = {{The Asymptotic Distribution of the MLE in High-dimensional Logistic Models: Arbitrary Covariance}},
url = {http://arxiv.org/abs/2001.09351},
year = {2020}
}
@article{Chernozhukov2022,
abstract = {Many economic and causal parameters depend on nonparametric or high dimensional first steps. We give a general construction of locally robust/orthogonal moment functions for GMM, where first steps have no effect, locally, on average moment functions. Using these orthogonal moments reduces model selection and regularization bias, as is important in many applications, especially for machine learning first steps. Also, associated standard errors are robust to misspecification when there is the same number of moment functions as parameters of interest.We use these orthogonal moments and cross‐fitting to construct debiased machine learning estimators of functions of high dimensional conditional quantiles and of dynamic discrete choice parameters with high dimensional state variables. We show that additional first steps needed for the orthogonal moment functions have no effect, globally, on average orthogonal moment functions. We give a general approach to estimating those additional first steps. We characterize double robustness and give a variety of new doubly robust moment functions. We give general and simple regularity conditions for asymptotic theory.},
archivePrefix = {arXiv},
arxivId = {1608.00033},
author = {Chernozhukov, Victor and Escanciano, Juan Carlos and Ichimura, Hidehiko and Newey, Whitney K. and Robins, James M.},
doi = {10.3982/ecta16294},
eprint = {1608.00033},
file = {::;::},
issn = {0012-9682},
journal = {Econometrica},
number = {4},
pages = {1501--1535},
title = {{Locally Robust Semiparametric Estimation}},
volume = {90},
year = {2022}
}
@article{Lundborg2022,
abstract = {Testing the significance of a variable or group of variables $X$ for predicting a response $Y$, given additional covariates $Z$, is a ubiquitous task in statistics. A simple but common approach is to specify a linear model, and then test whether the regression coefficient for $X$ is non-zero. However, when the model is misspecified, the test may have poor power, for example when $X$ is involved in complex interactions, or lead to many false rejections. In this work we study the problem of testing the model-free null of conditional mean independence, i.e. that the conditional mean of $Y$ given $X$ and $Z$ does not depend on $X$. We propose a simple and general framework that can leverage flexible nonparametric or machine learning methods, such as additive models or random forests, to yield both robust error control and high power. The procedure involves using these methods to perform regressions, first to estimate a form of projection of $Y$ on $X$ and $Z$ using one half of the data, and then to estimate the expected conditional covariance between this projection and $Y$ on the remaining half of the data. While the approach is general, we show that a version of our procedure using spline regression achieves what we show is the minimax optimal rate in this nonparametric testing problem. Numerical experiments demonstrate the effectiveness of our approach both in terms of maintaining Type I error control, and power, compared to several existing approaches.},
archivePrefix = {arXiv},
arxivId = {2211.02039},
author = {Lundborg, Anton Rask and Kim, Ilmun and Shah, Rajen D. and Samworth, Richard J.},
eprint = {2211.02039},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundborg et al. - 2022 - The Projected Covariance Measure for assumption-lean variable significance testing.pdf:pdf},
journal = {arXiv},
keywords = {double robustness},
mendeley-tags = {double robustness},
title = {{The Projected Covariance Measure for assumption-lean variable significance testing}},
url = {http://arxiv.org/abs/2211.02039},
year = {2022}
}
@article{Fan2023,
abstract = {We investigate the robustness of the model-X knockoffs framework with respect to the misspecified or estimated feature distribution. We achieve such a goal by theoretically studying the feature selection performance of a practically implemented knockoffs algorithm, which we name as the approximate knockoffs (ARK) procedure, under the measures of the false discovery rate (FDR) and family wise error rate (FWER). The approximate knockoffs procedure differs from the model-X knockoffs procedure only in that the former uses the misspecified or estimated feature distribution. A key technique in our theoretical analyses is to couple the approximate knockoffs procedure with the model-X knockoffs procedure so that random variables in these two procedures can be close in realizations. We prove that if such coupled model-X knockoffs procedure exists, the approximate knockoffs procedure can achieve the asymptotic FDR or FWER control at the target level. We showcase three specific constructions of such coupled model-X knockoff variables, verifying their existence and justifying the robustness of the model-X knockoffs framework.},
archivePrefix = {arXiv},
arxivId = {2307.04400},
author = {Fan, Yingying and Gao, Lan and Lv, Jinchi},
eprint = {2307.04400},
file = {::},
journal = {arXiv},
keywords = {FDR,coupling,false discovery,family-wise error rate control,feature selection,high dimensionality,knockoffs,knockoffs inference,rate control,robustness},
mendeley-tags = {FDR,knockoffs,robustness},
title = {{ARK: Robust Knockoffs Inference with Coupling}},
url = {http://arxiv.org/abs/2307.04400},
year = {2023}
}
@article{Belloni2013,
abstract = {In this article we study post-model selection estimators that apply ordinary least squares (OLS) to the model selected by first-step penalized estimators, typically Lasso. It is well known that Lasso can estimate the nonparametric regression function at nearly the oracle rate, and is thus hard to improve upon. We show that the OLS post-Lasso estimator performs at least as well as Lasso in terms of the rate of convergence, and has the advantage of a smaller bias. Remarkably, this performance occurs even if the Lasso-based model selection "fails" in the sense of missing some components of the "true" regression model. By the "true" model, we mean the best s-dimensional approximation to the nonparametric regression function chosen by the oracle. Furthermore, OLS post-Lasso estimator can perform strictly better than Lasso, in the sense of a strictly faster rate of convergence, if the Lasso-based model selection correctly includes all components of the "true" model as a subset and also achieves sufficient sparsity. In the extreme case, when Lasso perfectly selects the "true" model, the OLS post-Lasso estimator becomes the oracle estimator. An important ingredient in our analysis is a new sparsity bound on the dimension of the model selected by Lasso, which guarantees that this dimension is at most of the same order as the dimension of the "true" model. Our rate results are nonasymptotic and hold in both parametric and nonparametric models. Moreover, our analysis is not limited to the Lasso estimator acting as a selector in the first step, but also applies to any other estimator, for example, various forms of thresholded Lasso, with good rates and good sparsity properties. Our analysis covers both traditional thresholding and a new practical, data-driven thresholding scheme that induces additional sparsity subject to maintaining a certain goodness of fit. The latter scheme has theoretical guarantees similar to those of Lasso or OLS post-Lasso, but it dominates those procedures as well as traditional thresholding in a wide variety of experiments.},
author = {Belloni, Alexandre and Chernozhukov, Victor},
doi = {10.3150/11-BEJ410},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belloni, Chernozhukov - 2013 - Least squares after model selection in high-dimensional sparse models.pdf:pdf},
journal = {Bernoulli},
keywords = {Lasso,OLS post-Lasso,high-dimensional regression,post-model selection estimators,post-selection inference},
mendeley-tags = {high-dimensional regression,post-selection inference},
number = {2},
pages = {521--547},
title = {{Least squares after model selection in high-dimensional sparse models}},
volume = {19},
year = {2013}
}
@article{Rosenbaum1984,
abstract = {In observational studies, the distribution of treatment assignments is unknown, and therefore randomization tests are not generally applicable. However, permutation tests that condition on sample information about the treatment assignment mechanism can be applicable in observational studies, provided treatment assignment is strongly ignorable. These tests use the conditional distribution of the treatment assignments given a sufficient statistic for the unknown parameter of the propensity score. Several tests that are commonly used in observational studies are particular instances of this general procedure. Moreover, conditional permutation tests and covariance adjustment are closely related, in the sense that the conditional permutation distribution of the covariance adjusted difference leads to the same inferences as the conditional permutation distribution of the unadjusted difference of sample means. A backtrack algorithm is developed to permit efficient calculation of the exact conditional significance level, and two approximations are discussed. A clinical study of treatments for lung cancer is used to illustrate the technique. Conditional permutation tests extend previous large sample results on the propensity score by providing a basis for exact tests and confidence intervals in small observational studies when treatment assignment is strongly ignorable. {\textcopyright} 1984 Taylor & Francis Group, LLC.},
author = {Rosenbaum, Paul R.},
doi = {10.1080/01621459.1984.10478082},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Conditional inference,Ignorable treatment assignment,Logistic models,Observational studies,Randomization tests},
number = {387},
pages = {565--574},
title = {{Conditional permutation tests and the propensity score in observational studies}},
volume = {79},
year = {1984}
}
@book{VDV1998,
address = {Cambridge},
author = {{Van Der Vaart}, A. W.},
file = {::},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
publisher = {Cambridge University Press},
title = {{Asymptotic Statistics}},
year = {1998}
}
@article{Love2014,
abstract = {In comparative high-throughput sequencing assays, a fundamental task is the analysis of count data, such as read counts per gene in RNA-seq, for evidence of systematic changes across experimental conditions. Small replicate numbers, discreteness, large dynamic range and the presence of outliers require a suitable statistical approach. We present DESeq2, a method for differential analysis of count data, using shrinkage estimation for dispersions and fold changes to improve stability and interpretability of estimates. This enables a more quantitative analysis focused on the strength rather than the mere presence of differential expression. The DESeq2 package is available at http://www. bioconductor.org/packages/release/bioc/html/DESeq2.html. Background},
author = {Love, Michael I and Huber, Wolfgang and Anders, Simon},
doi = {10.1186/s13059-014-0550-8},
file = {::},
journal = {Genome Biology},
keywords = {gene expression,genetics},
mendeley-tags = {gene expression,genetics},
pages = {550},
title = {{Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2}},
volume = {15},
year = {2014}
}
@article{Weinstein2017,
abstract = {Knockoffs is a new framework for controlling the false discovery rate (FDR) in multiple hypothesis testing problems involving complex statistical models. While there has been great emphasis on Type-I error control, Type-II errors have been far less studied. In this paper we analyze the false negative rate or, equivalently, the power of a knockoff procedure associated with the Lasso solution path under an i.i.d. Gaussian design, and find that knockoffs asymptotically achieve close to optimal power with respect to an omniscient oracle. Furthermore, we demonstrate that for sparse signals, performing model selection via knockoff filtering achieves nearly ideal prediction errors as compared to a Lasso oracle equipped with full knowledge of the distribution of the unknown regression coefficients. The i.i.d. Gaussian design is adopted to leverage results concerning the empirical distribution of the Lasso estimates, which makes power calculation possible for both knockoff and oracle procedures.},
archivePrefix = {arXiv},
arxivId = {1712.06465},
author = {Weinstein, Asaf and Barber, Rina and Candes, Emmanuel},
eprint = {1712.06465},
file = {::},
journal = {arXiv},
title = {{A power analysis for knockoffs under Gaussian designs}},
url = {http://arxiv.org/abs/1712.06465},
year = {2017}
}
@article{Katsevich2020a,
abstract = {For testing conditional independence (CI) of a response $Y$ and a predictor $X$ given covariates $Z$, the recently introduced model-X (MX) framework has been the subject of active methodological research, especially in the context of MX knockoffs and their successful application to genome-wide association studies. In this paper, we build a theoretical foundation for the MX CI problem, yielding quantitative explanations for empirically observed phenomena and novel insights to guide the design of MX methodology. We focus our analysis on the conditional randomization test (CRT), whose validity conditional on $Y,Z$ allows us to view it as a test of a point null hypothesis involving the conditional distribution of $X$. We use the Neyman-Pearson lemma to derive an intuitive most-powerful CRT statistic against a point alternative as well as an analogous result for MX knockoffs. We define MX analogs of $t$- and $F$- tests and derive their power against local semiparametric alternatives using Le Cam's local asymptotic normality theory, explicitly capturing the prediction error of the underlying machine learning procedure. Importantly, all our results hold conditionally on $Y,Z$, almost surely in $Y,Z$. Finally, we define nonparametric notions of effect size and derive consistent estimators inspired by semiparametric statistics. Thus, this work forms explicit, and underexplored, bridges from MX to both classical statistics (testing) and modern causal inference (estimation).},
archivePrefix = {arXiv},
arxivId = {2005.05506},
author = {Katsevich, Eugene and Ramdas, Aaditya},
eprint = {2005.05506},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Katsevich, Ramdas - 2020 - On the power of conditional independence testing under model-X.pdf:pdf},
journal = {Electronic Journal of Statistics},
number = {2},
pages = {6348--6394},
title = {{On the power of conditional independence testing under model-X}},
url = {http://arxiv.org/abs/2005.05506},
volume = {16},
year = {2022}
}
@article{Chernozhukov2018,
abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter $\theta$0 in the presence of high-dimensional nuisance parameters $\eta$0. We depart from the classical setting by allowing for $\eta$0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate $\eta$0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating $\eta$0 cause a heavy bias in estimators of $\theta$0 that are obtained by naively plugging ML estimators of $\eta$0 into estimating equations for $\theta$0. This bias results in the naive estimator failing to be N-1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest $\theta$0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate $\theta$0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N-1 -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
doi = {10.1111/ectj.12097},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and structural parameters(2).pdf:pdf},
issn = {1368423X},
journal = {Econometrics Journal},
number = {1},
pages = {C1--C68},
title = {{Double/debiased machine learning for treatment and structural parameters}},
volume = {21},
year = {2018}
}
@article{Majerek2005a,
author = {Majerek, Dariusz and Nowak, Wioletta and Zieba, W},
file = {::},
journal = {Int. J. Pure Appl. Math},
number = {2},
pages = {143--156},
title = {{Conditional strong law of large number}},
volume = {20},
year = {2005}
}
@book{svm2008,
author = {Steinwart, Ingo and Christmann, Andreas},
file = {::},
isbn = {2013206534},
keywords = {kernel method},
mendeley-tags = {kernel method},
publisher = {Springer},
title = {{Support-Vector-Machines}},
year = {2008}
}
@book{Wainwright2019,
abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
author = {Wainwright, Martin J.},
booktitle = {High-Dimensional Statistics: A Non-Asymptotic Viewpoint},
doi = {10.1017/9781108627771},
keywords = {Martin2019},
mendeley-tags = {Martin2019},
title = {{High-dimensional statistics: A non-asymptotic viewpoint}},
year = {2019}
}
@article{Nowak2005,
abstract = {The aim of this paper is to investigate relations between different types of conditional convergence. Results presented in this paper generalize theorems obtained by P. Fernandez [2] and A. R. Padmanabhan [5].},
author = {Nowak, Wioletta and Zi{\c{e}}ba, Wies{\l}aw},
file = {::},
journal = {Annales Universitatis Mariae Curie-Sklodowska Lublin-Polonia},
keywords = {conditional expectation,convergence in probability},
pages = {97--105},
title = {{Types of conditional convergence}},
url = {http://math.umcs.lublin.pl/annales/2005/10.pdf},
volume = {59},
year = {2005}
}
@article{Kennedy2020,
abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
archivePrefix = {arXiv},
arxivId = {2004.14497},
author = {Kennedy, Edward H.},
eprint = {2004.14497},
file = {::},
keywords = {causal inference,conditional effects,heterogeneous treatment effects,influence function,minimax rate,nonparametric regression},
mendeley-tags = {causal inference,heterogeneous treatment effects},
pages = {1--35},
title = {{Optimal doubly robust estimation of heterogeneous causal effects}},
url = {http://arxiv.org/abs/2004.14497},
year = {2020}
}
@article{Smucler2019,
abstract = {We consider inference about a scalar parameter under a non-parametric model based on a one-step estimator computed as a plug in estimator plus the empirical mean of an estimator of the parameter's influence function. We focus on a class of parameters that have influence function which depends on two infinite dimensional nuisance functions and such that the bias of the one-step estimator of the parameter of interest is the expectation of the product of the estimation errors of the two nuisance functions. Our class includes many important treatment effect contrasts of interest in causal inference and econometrics, such as ATE, ATT, an integrated causal contrast with a continuous treatment, and the mean of an outcome missing not at random. We propose estimators of the target parameter that entertain approximately sparse regression models for the nuisance functions allowing for the number of potential confounders to be even larger than the sample size. By employing sample splitting, cross-fitting and $\ell_1$-regularized regression estimators of the nuisance functions based on objective functions whose directional derivatives agree with those of the parameter's influence function, we obtain estimators of the target parameter with two desirable robustness properties: (1) they are rate doubly-robust in that they are root-n consistent and asymptotically normal when both nuisance functions follow approximately sparse models, even if one function has a very non-sparse regression coefficient, so long as the other has a sufficiently sparse regression coefficient, and (2) they are model doubly-robust in that they are root-n consistent and asymptotically normal even if one of the nuisance functions does not follow an approximately sparse model so long as the other nuisance function follows an approximately sparse model with a sufficiently sparse regression coefficient.},
archivePrefix = {arXiv},
arxivId = {1904.03737},
author = {Smucler, Ezequiel and Rotnitzky, Andrea and Robins, James M.},
eprint = {1904.03737},
file = {::},
journal = {arXiv},
title = {{A unifying approach for doubly-robust L1 regularized estimation of causal contrasts}},
url = {http://arxiv.org/abs/1904.03737},
year = {2019}
}
@article{bulinski2017conditional,
abstract = {Despite the prevalence of synchronization analysis on complex dynamical networks, little attention was paid to the problem of estimating their regions of attraction. This paper addresses the issue of estimating the region of attraction of an equilibrium point of a complex dynamical network, and briefly analyzes the network stability. A sufficient condition and a necessary condition are first established for the asymptotical stability of the network equilibrium point. Then, a general technique for region-of-attraction estimation is developed by combining the network structure and the node dynamics. In order to avoid the troublesome parameter selection in general region-of-attraction estimation, second-order estimation is solved under a mild additional condition. Examples are provided to verify the theoretical estimations.},
author = {Bulinski, A. V.},
file = {::},
journal = {Theory of Probability and its Applications},
keywords = {080733565,1,10,1137,62f15,62g08,65c05,ams subject classifications,bayesian,doi,introduction and examples,monte carlo,nonparametric regression,recognize the stochas-,scientists have come to,uncertainty quantification},
number = {4},
pages = {613--631},
title = {{Conditional central limit theorem}},
volume = {61},
year = {2017}
}
@techreport{Fei2021,
abstract = {The focus of modern biomedical studies has gradually shifted to explanation and estimation of joint effects of high dimensional predictors on disease risks. Quantifying uncertainty in these estimates may provide valuable insight into prevention strategies or treatment decisions for both patients and physicians. High dimensional inference, including confidence intervals and hypothesis testing, has sparked much interest. While much work has been done in the linear regression setting, there is a lack of literature on inference for high dimensional generalized linear models. We propose a novel and computationally feasible method, which accommodates a variety of outcome types, including normal, binomial, and Poisson data. We use a "splitting and smoothing" approach, which splits samples into two parts, performs variable selection using one part and conducts partial regression with the other part. Averaging the estimates over multiple random splits, we obtain the smoothed estimates, which are numerically stable. We show that the estimates are consistent, asymp-totically normal, and construct confidence intervals with proper coverage probabilities for all predictors. We examine the finite sample performance of our method by comparing it with the existing methods and applying it to analyze a lung cancer cohort study.},
author = {Fei, Zhe and Li, Yi},
booktitle = {Journal of Machine Learning Research},
file = {::},
keywords = {Confidence intervals,dimension reduction,high dimensional inference for GLMs,sparsity,sure screening},
pages = {1--32},
title = {{Estimation and Inference for High Dimensional Generalized Linear Models: A Splitting and Smoothing Approach}},
url = {http://jmlr.org/papers/v22/19-132.html.},
volume = {22},
year = {2021}
}
@article{Nie2021,
abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical applications, such as personalized medicine and optimal resource allocation. In this article we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. First, we estimate marginal effects and treatment propensities to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. The proposed approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: in both steps, any loss-minimization method can be employed, such as penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross-validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property. Even when the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle with prior knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation set-ups, and observe promising performance relative to existing baselines.},
archivePrefix = {arXiv},
arxivId = {1712.04912},
author = {Nie, X and Wager, S},
doi = {10.1093/biomet/asaa076},
eprint = {1712.04912},
file = {::},
issn = {0006-3444},
journal = {Biometrika},
keywords = {boosting,causal inference,empirical risk minimization,heterogeneous treatment effects,kernel regression,penalized regression},
mendeley-tags = {causal inference,heterogeneous treatment effects},
number = {2},
pages = {299--319},
title = {{Quasi-oracle estimation of heterogeneous treatment effects}},
volume = {108},
year = {2021}
}
@article{Barber2020,
author = {Barber, Rina Foygel and Janson, Lucas},
file = {::},
journal = {Annals of Statistics},
keywords = {approximate sufficiency,co-sufficiency,conditional independence testing,conditional ran-,conditional randomization test,domization test,goodness-of-fit test,high-dimensional inference,high-dimensional regression,model-X,model-x},
mendeley-tags = {conditional randomization test,high-dimensional regression,model-X},
number = {5},
pages = {2514--2544},
title = {{Testing goodness-of-fit and conditional independence with approximate co-sufficient sampling}},
volume = {50},
year = {2022}
}
@article{Chernozhukov2013,
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1214/13-AOS1161},
file = {::;::},
journal = {The Annals of Statistics},
keywords = {asymptotics},
mendeley-tags = {asymptotics},
number = {6},
pages = {2786--2819},
title = {{Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors}},
volume = {41},
year = {2013}
}
@article{Marohn2002,
abstract = {Let P$\kappa$,$\lambda$ be k-parametric family of distributions, where $\kappa$ ∈ ℝ is a one-dimensional parameter of interest and $\lambda$ ∈ ℝk-1 is a nuisance parameter. Assuming L2-difierentiability of the underlying model, we give conditions for the asymptotic optimality of locally most powerful (LMP) tests with estimated nuisance parameter. Various examples as scale-location families are discussed. Copyright {\textcopyright} 2002 by Marcel Dekker, Inc.},
author = {Marohn, Frank},
doi = {10.1081/STA-120002852},
file = {::},
issn = {03610926},
journal = {Communications in Statistics - Theory and Methods},
keywords = {Efficient estimator sequence,Local asymptotic normality (LAN),Optimal test sequence,Score test,nuisance parameters,testing},
mendeley-tags = {nuisance parameters,testing},
number = {3},
pages = {337--349},
title = {{A comment on locally most powerful tests in the presence of nuisance parameters}},
volume = {31},
year = {2002}
}
@article{Liu2022a,
abstract = {We consider the problem of conditional independence testing: given a response $Y$ and covariates $(X,Z)$, we test the null hypothesis that $Y {\perp\!\!\!\perp} X \mid Z$. The conditional randomization test was recently proposed as a way to use distributional information about $X\mid Z$ to exactly and nonasymptotically control Type-I error using any test statistic in any dimensionality without assuming anything about $Y\mid (X,Z)$. This flexibility, in principle, allows one to derive powerful test statistics from complex prediction algorithms while maintaining statistical validity. Yet the direct use of such advanced test statistics in the conditional randomization test is prohibitively computationally expensive, especially with multiple testing, due to the requirement to recompute the test statistic many times on resampled data. We propose the distilled conditional randomization test, a novel approach to using state-of-the-art machine learning algorithms in the conditional randomization test while drastically reducing the number of times those algorithms need to be run, thereby taking advantage of their power and the conditional randomization test's statistical guarantees without suffering the usual computational expense. In addition to distillation, we propose a number of other tricks, like screening and recycling computations, to further speed up the conditional randomization test without sacrificing its high power and exact validity. Indeed, we show in simulations that all our proposals combined lead to a test that has similar power to most powerful existing conditional randomization test implementations, but requires orders of magnitude less computation, making it a practical tool even for large datasets. We demonstrate these benefits on a breast cancer dataset by identifying biomarkers related to cancer stage.},
author = {Liu, Molei and Katsevich, Eugene and Janson, Lucas and Ramdas, Aaditya},
doi = {10.1093/biomet/asab039},
file = {::},
issn = {14643510},
journal = {Biometrika},
keywords = {Conditional independence test,Conditional randomization test,High-dimensional inference,Machine learning,Model-X},
number = {2},
pages = {277--293},
title = {{Fast and powerful conditional randomization testing via distillation}},
volume = {109},
year = {2022}
}
@article{Tansey2018,
abstract = {We consider the problem of feature selection using black box predictive models. For example, high-throughput devices in science are routinely used to gather thousands of features for each sample in an experiment. The scientist must then sift through the many candidate features to find explanatory signals in the data, such as which genes are associated with sensitivity to a prospective therapy. Often, predictive models are used for this task: the model is fit, error on held out data is measured, and strong performing models are assumed to have discovered some fundamental properties of the system. A model-specific heuristic is then used to inspect the model parameters and rank important features, with top features reported as "discoveries." However, such heuristics provide no statistical guarantees and can produce unreliable results. We propose the holdout randomization test (HRT) as a principled approach to feature selection using black box predictive models. The HRT is model agnostic and produces a valid p-value for each feature, enabling control over the false discovery rate (or Type I error) for any predictive model. Further, the HRT is computationally efficient and, in simulations, has greater power than a competing knockoffs-based approach. Code is available at https://github.com/tansey/hrt.},
archivePrefix = {arXiv},
arxivId = {1811.00645},
author = {Tansey, Wesley and Veitch, Victor and Zhang, Haoran and Rabadan, Raul and Blei, David M.},
eprint = {1811.00645},
file = {:C\:/Users/abch/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tansey et al. - 2018 - The Holdout Randomization Test Principled and Easy Black Box Feature Selection.pdf:pdf},
journal = {arXiv},
keywords = {conditional randomization test,high-dimensional regression,variable selection},
mendeley-tags = {conditional randomization test,high-dimensional regression,variable selection},
title = {{The Holdout Randomization Test: Principled and Easy Black Box Feature Selection}},
url = {http://arxiv.org/abs/1811.00645},
year = {2018}
}
@article{Celentano2021,
abstract = {We consider the problem of estimating a low-dimensional parameter in high-dimensional linear regression. Constructing an approximately unbiased estimate of the parameter of interest is a crucial step towards performing statistical inference. Several authors suggest to orthogonalize both the variable of interest and the outcome with respect to the nuisance variables, and then regress the residual outcome with respect to the residual variable. This is possible if the covariance structure of the regressors is perfectly known, or is sufficiently structured that it can be estimated accurately from data (e.g., the precision matrix is sufficiently sparse). Here we consider a regime in which the covariate model can only be estimated inaccurately, and hence existing debiasing approaches are not guaranteed to work. When errors in estimating the covariate model are correlated with errors in estimating the linear model parameter, an incomplete elimination of the bias occurs. We propose the Correlation Adjusted Debiased Lasso (CAD), which nearly eliminates this bias in some cases, including cases in which the estimation errors are neither negligible nor orthogonal. We consider a setting in which some unlabeled samples might be available to the statistician alongside labeled ones (semi-supervised learning), and our guarantees hold under the assumption of jointly Gaussian covariates. The new debiased estimator is guaranteed to cancel the bias in two cases: (1) when the total number of samples (labeled and unlabeled) is larger than the number of parameters, or (2) when the covariance of the nuisance (but not the effect of the nuisance on the variable of interest) is known. Neither of these cases is treated by state-of-the-art methods.},
archivePrefix = {arXiv},
arxivId = {2107.14172},
author = {Celentano, Michael and Montanari, Andrea},
eprint = {2107.14172},
file = {::},
journal = {arXiv},
keywords = {AMP,Lasso,debiased lasso,double robustness,high-dimensional regression},
mendeley-tags = {AMP,Lasso,debiased lasso,double robustness,high-dimensional regression},
title = {{CAD: Debiasing the Lasso with inaccurate covariate model}},
url = {http://arxiv.org/abs/2107.14172},
year = {2021}
}
@article{Karoui2018,
abstract = {We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where p < n but p/n is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of $\beta$ (where $\beta$ is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression—residual bootstrap and pairs bootstrap—give very poor inference on $\beta$ as the ratio p/n grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio p/n grows. We also show that the jackknife resampling technique for estimating the variance of $\beta$ severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods.},
author = {Karoui, Noureddine El and Purdom, Elizabeth},
file = {::},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Bootstrap,High-dimensional inference,Random matrices,Resampling,bootstrap,high-dimensional regression},
mendeley-tags = {bootstrap,high-dimensional regression},
pages = {1--66},
title = {{Can we trust the bootstrap in high-dimensions? The case of linear models}},
volume = {19},
year = {2018}
}
@article{Javanmard2014,
abstract = {Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical significance as confidence intervals or p-values for these models. We consider here high-dimensional linear regression problem, and propose an efficient algorithm for constructing confidence intervals and p-values. The resulting confidence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a 'de-biased' version of regularized M-estimators. The new construction improves over recent work in the field in that it does not assume a special structure on the design matrix. We test our method on synthetic data and a high-throughput genomic data set about riboflavin production rate, made publicly available by B{\"{u}}hlmann et al. (2014).},
author = {Javanmard, Adel and Montanari, Andrea},
file = {::},
journal = {Journal of Machine Learning Research},
keywords = {LASSO,bias of an estimator,confidence intervals,high-dimensional models,high-dimensional regression,hypothesis testing},
mendeley-tags = {confidence intervals,high-dimensional regression},
pages = {2869--2909},
title = {{Confidence Intervals and Hypothesis Testing for High-Dimensional Regression}},
volume = {15},
year = {2014}
}
@article{Sur2019,
abstract = {Students in statistics or data science usually learn early on that when the sample size n is large relative to the number of variables p, fitting a logistic model by the method of maximum likelihood produces estimates that are consistent and that there are well-known formulas that quantify the variability of these estimates which are used for the purpose of statistical inference. We are often told that these calculations are approximately valid if we have 5 to 10 observations per unknown parameter. This paper shows that this is far from the case, and consequently, inferences produced by common software packages are often unreliable. Consider a logistic model with independent features in which n and p become increasingly large in a fixed ratio. We prove that (i) the maximum-likelihood estimate (MLE) is biased, (ii) the variability of the MLE is far greater than classically estimated, and (iii) the likelihood-ratio test (LRT) is not distributed as a $\chi$2. The bias of the MLE yields wrong predictions for the probability of a case based on observed values of the covariates. We present a theory, which provides explicit expressions for the asymptotic bias and variance of the MLE and the asymptotic distribution of the LRT. We empirically demonstrate that these results are accurate in finite samples. Our results depend only on a single measure of signal strength, which leads to concrete proposals for obtaining accurate inference in finite samples through the estimate of this measure.},
archivePrefix = {arXiv},
arxivId = {1803.06964},
author = {Sur, Pragya and Cand{\`{e}}s, Emmanuel J.},
doi = {10.1073/pnas.1810420116},
eprint = {1803.06964},
file = {::},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {AMP,High-dimensional inference,Likelihood-ratio test,Logistic regression,Maximum-likelihood estimate,high-dimensional regression},
mendeley-tags = {AMP,high-dimensional regression},
number = {29},
pages = {14516--14525},
pmid = {31262828},
title = {{A modern maximum-likelihood theory for high-dimensional logistic regression}},
volume = {116},
year = {2019}
}
@article{Sesia2022,
archivePrefix = {arXiv},
arxivId = {arXiv:2205.08653v1},
author = {Sesia, Matteo and Sun, Tianshu},
eprint = {arXiv:2205.08653v1},
file = {::},
journal = {arXiv},
title = {{Individualized conditional independence testing under model-X with heterogeneous samples and interactions}},
year = {2022}
}
@article{Hines2021,
abstract = {Evaluation of treatment effects and more general estimands is typically achieved via parametric modelling, which is unsatisfactory since model misspecification is likely. Data-adaptive model building (e.g. statistical/machine learning) is commonly employed to reduce the risk of misspecification. Naive use of such methods, however, delivers estimators whose bias may shrink too slowly with sample size for inferential methods to perform well, including those based on the bootstrap. Bias arises because standard data-adaptive methods are tuned towards minimal prediction error as opposed to e.g. minimal MSE in the estimator. This may cause excess variability that is difficult to acknowledge, due to the complexity of such strategies. Building on results from non-parametric statistics, targeted learning and debiased machine learning overcome these problems by constructing estimators using the estimand's efficient influence function under the non-parametric model. These increasingly popular methodologies typically assume that the efficient influence function is given, or that the reader is familiar with its derivation. In this paper, we focus on derivation of the efficient influence function and explain how it may be used to construct statistical/machine-learning-based estimators. We discuss the requisite conditions for these estimators to perform well and use diverse examples to convey the broad applicability of the theory.},
archivePrefix = {arXiv},
arxivId = {2107.00681},
author = {Hines, Oliver and Dukes, Oliver and Diaz-Ordaz, Karla and Vansteelandt, Stijn},
eprint = {2107.00681},
file = {::},
journal = {arXiv},
keywords = {ble machine learning,causal machine learning,data-adaptive estimation,dou-,double robustness,nonparametric methods,post-selection inference},
mendeley-tags = {double robustness},
pages = {1--27},
title = {{Demystifying statistical learning based on efficient influence functions}},
url = {http://arxiv.org/abs/2107.00681},
year = {2021}
}
@misc{Donald1994,
abstract = {This paper discusses estimation of the semilinear model E[y | x, z] = x′$\beta$ + g(z) using series approximations to the unknown function g(z) under much weaker conditions than heretofore given in the literature. In particular, we allow for z being multidimensional and to have a discrete distribution, features often present in applications. In addition, the smoothness conditions are quite weak: it will suffice for √n consistency of $\beta$̂ that the modulus of continuity of g(z) and E[x | z] be higher than one-fourth the dimension of z and that the number of terms be chosen appropriately. {\textcopyright} 1994 Academic Press, Inc.},
author = {Donald, S. G. and Newey, W. K.},
booktitle = {Journal of Multivariate Analysis},
doi = {10.1006/jmva.1994.1032},
file = {::},
issn = {10957243},
keywords = {Partly linear model,Power series approximation,Semiparametric model,Spline approximation},
number = {1},
pages = {30--40},
title = {{Series estimation of semilinear models}},
volume = {50},
year = {1994}
}
@article{Davidson2003,
abstract = {Economics; Econometrics; Asymptotics},
author = {Davidson, James},
doi = {10.1093/0198774036.001.0001},
file = {::},
isbn = {0198774036},
journal = {Stochastic Limit Theory},
title = {{Stochastic Limit Theory}},
year = {2003}
}

@article{supplementary,
  author          = {Niu, Ziang and Charaborty, Abhinav, and Dukes, Oliver and Katsevich, Eugene},
  title = {Supplement to “Reconciling model-X and doubly robust approaches to conditional independence testing”},
  year            = {2023}
}